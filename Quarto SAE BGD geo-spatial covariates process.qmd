---
title: "Guide to processing of geo-spatial covariates to be used in poverty modelling - an example of Bangladesh"
author: "Ida Brzezinska"
format: html
editor: visual
---

## 1. Introduction

# Contents Page

## Section A

## Section B

### Introduction:

Page number:

### Estimation:

Page number:

### Conclusion / Caveats:

Page number:

### Glossary

Page number:

# Introduction

**Why is it so important to learn more about the spatial distribution of poverty?**

-   More than 700 million people or 9.3% of the population live on less than \$2.15 a day.
-   Approximately half the world's population live on less than \$6.85 a day.
-   Despite global inequality declining, inequality within countries is rising so it is increasingly important for policymakers to appreciate the spatial distribution of wealth across a whole country.

Social protection programs and appropriate policy decisions are important drivers of poverty reduction but they need suitable data in order to effectively target those who need help the most. Policy makers therefore require the most current statistics as well as figures that give an idea of poverty distribution across the whole country. This method of small area estimation can help solve both of these issues.

#PJ: Comment from Paul.

Poverty estimates are a crucial measure for policymakers and development practitioners to understand the extent of poverty in a country and design effective poverty alleviation programs. This guide will tell you how to produce these estimates.

**The Fay-Herriot (FH) Model**

The FH model in R can be used to produce out-of-sample poverty estimates.

The FH model uses a combination of household data and geospatial data in order to generate estimates of poverty rates.

It does this by fitting a model on the in-sample observations by using the household survey responses and learning which geospatial covariates have the most predictive power on the wealth index variable in the household survey. The model can then estimate the poverty rates for out of sample households using just the geospatial data.

This technique is demonstrated by [Blumenstock et al., (2022)](https://pubmed.ncbi.nlm.nih.gov/35017299/), who develop relative wealth and poverty microestimates for all 135 low and middle income countries (LMICs).

**Why do we use geospatial data rather than census data?**

-   Previously, small area estimation poverty estimates were generated using a combination of household survey and census data, which is only collected every 10 years. However, a combination of household survey data and geospatial data allows you to generate poverty estimates much more frequently than was possible previously.
-   Geospatial data is open source and can provide more precise and up-to-date information about covariates than census data. This also makes the estimation process cheaper if you use geospatial data.
-   Geospatial data does not have a problem of finding statistics in rural areas which are often hard to reach in traditional surveys.

**Geospatial has all these advantages whilst still offering sufficient predictive power**

-   Geospatial data is available at a high spatial resolution, which can be matched with the geo-location of households in order to allow prediction of household welfare from the co-ordinates.
-   This information can then be used to estimate poverty rates for that household by using geospatial information such as land use, population demographics and climatic variables.
-   The FH model using geospatial data, when compared to ground-truth data in Togo using a nationally representative sample of 6,172 households found that the model explained 84% of the variation in wealth. This compares favourably to other state-of-the-art models.

**The guide**

In this guide, we will take you through the steps of using the Fay-Herriot model in R to carry out small area estimation to predict poverty rates. This will enable you to replicate this prediction in other developing countries using just geospatial data and a household survey that includes a wealth index. We use the DHS household survey that includes data on over 90 different countries for ease of replication for the user in other countries. We will explain how the model works, the data we use, and why we might choose to use this method over traditional approaches. By the end of this guide, you will be able to produce accurate and up-to-date poverty estimates for small areas in developing countries, which can inform policy decisions and improve poverty alleviation efforts.

In this guide we use the Fay-Herriot model vs other predictive models since the FH model estimates poverty rates using the agglomerated average wealth index of each area (and is therefore known as an area-level model) rather than other models such as the EBP (empirical best predictor) which estimates the poverty rates by generating an estimate for every single household (known as unit-level). This makes the results easier to replicate in a large variety of countries since there is often a problem in obtaining unit-level data due to data protection laws.

**Why we use the relative wealth index as our measure of poverty**

We use the DHS "relative wealth index" as our ground-truth measure of wealth and poverty. This means that our machine-learning algorithms are being trained to reconstruct a specific, asset-based relative wealth index - albeit at a much finer spatial resolution and in areas where DHS surveys did not occur. This is because we believe the DHS version of a relative wealth index is the best publicly available instrument for consistently measuring wealth across a large number of developing countries.

The wealth index is a composite measure of a household's cumulative living standard. The wealth index is calculated using easy-to-collect data on a household's ownership of selected assets, such as televisions and bicycles; materials used for housing construction; and types of water access and sanitation facilities. The wealth index is generated with a statistical procedure known as principal components analysis. The wealth index places individual households on a continuous scale of relative wealth. However, since it posits a specific, asset-based definition of wealth, it does not necessarily capture a broader notion of human development.

Our decision to focus on estimating asset-based wealth, rather than a different measure of socioeconomic status, was motivated by several considerations:

-   In developing economies, where large portions of the population do not earn formal wages, measures of income are notoriously unreliable. Instead, researchers and policymakers rely on asset-based wealth indexes or measures of consumption expenditures.
-   Between these two, wealth is much less time consuming to record in a survey. As a result, wealth data is more commonly collected in a standardized format for a large number of countries.
-   This therefore lends itself to the FH method being easily replicated and reproduced in a wide variety of case studies in less developed economies around the world.

### Steps of the guide

Using the Fay-Herriot model as a method to produce out of sample estimates of poverty initially seems like quite an involved process. However, the main steps are quite simple. You first gather your geospatial data and download the DHS household survey responses from online. You then calculate the direct estimates of poverty in the country using the DHS household survey. The reason you cannot simply use the numbers in the survey is because DHS employs a sampling strategy to gather their data and this needs to be accounted for and the numbers need to be adjusted in order to make the estimates as precise as possible. You then select the most significant geospatial covariates at predicting poverty and use them in a Fay-Herriot model to generate your out of sample poverty estimates. These estimates are then used to visualise the spatial distribution of poverty in the country.

The remainder of the steps in the process are used to ensure the robustness and precision of the estimates. The FH model has a few assumptions that you need to check are not violated, especially if you are using the model to predict out of sample estimates. It is therefore vital that these steps are followed if the estimates produced are to be used to guide policy decisions in the future.

Step 1: Gathering the geospatial data and compiling into a harmonised dataset.

You can either start with the geospatial co-ordinates you generated in section A or use our pre-processed dataframe which is available [here.]{.underline} Section A of our guide explains how to gather the geospatial data in detail.

Step 2: Generating direct estimates of the wealth index and their sampling variance for the in-sample observations.

Surveys often employ complex sampling designs, with elements of stratification and multistage sampling with unequal probabilities. In order to account for this, we need to calculate direct estimates and estimates of their variances for the wealth index in the DHS household dataset. This is in order to prevent bias being introduced into the estimation.

For this we use the survey() package in R. This takes into account the bias introduced by the DHS' sampling strategy and allows you to use these statistics to train your model.

Step 3: Merging the geospatial data to the household data and your direct estimates.

Now we need to combine the geospatial data from Section A of the guide, the DHS household survey and the direct estimates and their variances that you generated in Section B of the process. In order to combine this information into one big dataset we need a unique common identifier and for this we merge the observations using the unique domain identifier.

Step 4: Selecting the geospatial covariates with the most predictive power

The initial large dataframe with over 150 geospatial variables is unwieldy and following the 'principle of parsimony' we want fewer variables. We therefore initially trim the model to include fewer covariates. This helps to prevent issues such as overfitting. For this we use a selection criterion (AIC/BIC/KIC) to find only the geospatial covariates with the highest predictive power on predicting the wealth estimates.

We perform a BIC stepwise selection criterion using all of the available geospatial covariates to see which 15-20 variables perform the best in predicting the level of wealth in an area.

Step 5: Generating the model and checking model assumptions.

We now use the geospatial covariates selected in step 4 by the selection criterion in a cursory model in order to test the key assumptions of the model and assess the predictive power. For example, the model assumes normality of the error terms which, if violated, introduces bias into the estimation so it is necessary to show that all the necessary assumptions hold.

If the assumptions do not hold with the model in its current format then it is important to carry out a transformation on the target variable. We use a variety of visual aids such as plots as well as regression tests to ensure that we are satisfied that these assumptions hold after a transformation.

Step 6: Carry out the FH model prediction for our out of sample estimates.

In this step we carry out the FH modelling procedure in order to generate our out of sample wealth estimates. The FH model adopts a bootstrap approach in order to predict out of sample estimates. This is an iterative resampling method which provides estimates of variability and uncertainty without requiring strong assumptions about the data distribution.

The FH model has been trained on in-sample domains which have both geospatial and household data available. However, by definition, only geospatial data is available in out-of-sample domains. By learning the relationship between the most significant geospatial covariates and the poverty rates of an area (as determined in step 4), the model can estimate what the poverty rates would be in an area that has not been surveyed in the DHS.

Step 7: Comparison of FH model estimates with the ground-truth direct estimates.

In this step we compare the direct point poverty estimates we generated in step 2 using the DHS data with the FH model estimates that we generated in step 6. This is done both graphically and with a Brown test. This gives some indication of the precision of the model. If we are satisfied with the similarity of the estimates, we can move on to the final stage.

Step 8: Visualisation of the results.

The final step of the process is visualising the results. To do this we have to download a Bangladesh shapefile from online which includes the co-ordinates of the country's border lines and includes the polygon shapes for each upazila. We merge the dataset using the unique upazila district code and then visualise the relative wealth in each domain by using a gradient colour scheme.

# Section A: Producing geo-spatial covariates

::: callout-note
## Glossary

**Raster data** is a collection of cells (or pixels) organised into rows and columns (called a grid), defined by its location in space. The raster is the pattern of locations that together - as a grid - cover a geographic area. Cells have values that carry information about those locations, such as temperature or elevation. Values can be continuous (e.g. a range of temperature values), or categorical (e.g. different categories of land use).

**Vector data** represents structures on the Earth's surface that are defined by discrete locations (x,y) called vertices. Depending on how the vertices (x,y) are organised, we can divide vector data types into: **points, lines, and polygons**. Vector data is commonly used to represent structures such as: road networks, rivers, bridges, buildings, administrative areas.

**Extent**: the maximum and minimum values of longitude and latitude that define the boundaries of our spatial structure.

**Map projections** try to portray the surface of the earth or a portion of the earth on a flat piece of paper or computer screen. 

**Coordinate Reference System (CRS)** - defines, with the help of coordinates, how the two-dimensional projected map in our GIS is related to real places on Earth.

**Geographic Information System (GIS)** - a system that creates, manages, analyses, and maps data. GIS connects data to a map, integrating location data (where things are), with all types of descriptive information (what things are like there).

**Spatial resolution** of a raster describes the area that each pixel (or observation) covers. For example, a 1km x 1km spatial resolution will be a grid where each grid cell covers and area of 1km x 1km. The smaller the area covered, the higher the spatial resolution (i.e. the more we "zoom in").

**Resampling** - changing the spatial resolution of raster data, i.e. the size of cells that give us information about a geographical area. If we move from a lower to a higher spatial resolution, our pixels (or cells) become smaller - so that we are able to see more granularity in the data. For instance, moving from a raster where each cell covers an area of 30km x 30km to a raster where each grid cell covers only 1km x 1km would mean resampling to a higher spatial resolution, with more density points in space that carry information. There are various algorithms for resampling. Some examples we will be using, such as neareast neighbour interpolation and the "sum" algorithm, are described in Section 3.
:::

## 1. Introduction

Section A is our first step into poverty modelling in Bangladesh. Here, we will obtain and transform various geo-spatial data sets (such as accessibility, nightlights intensity, demographic maps, and topography) into predictors of poverty rates. This data has been sourced through a combination of ground surveys and remote sensing (collected by satellites and remote sensors orbiting the Earth). Geo-spatial variables are important inputs into poverty estimation models. Outputs from such models fill spatial and temporal gaps that exist between official poverty statistics. This is due to the many advantages of **remote sensed** geo-spatial data:

-   Has wide coverage and typically includes data from low-and-middle-income countries (LMICs).

-   Covers areas that would otherwise be difficult to access, such as mountain ranges, islands, and conflict zones.

-   Can be obtained at a high spatial resolution - in some cases as high as 30m x 30m (which is the resolution of the population density maps that we will be using).

-   Measurements can be collected frequently. For instance, the [ERA5](https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5) climate data set from the European Centre for Medium-Range Weather Foecasts (ECMWF) contains measures of temperature for every hour. This is a particulary exciting aspect as in theory you could have a dynamic poverty map which updates itself as new geo-spatial data becomes available!

The main goal of Section A will be to produce a "**harmonised geo-spatial covariate raster for poverty modelling**". In other words, layers of geo-spatial information in a raster format (a collection of pixels) for each small area that can be used to estimate poverty. In our case, small areas will be **upazilas** in Bangladesh, which is the administrative level 3.

Imagine that each upazila is divided into pixels. Each pixel will contain some geo-spatial information: slope, travel time to healthcare facilities, distance to nearest water source etc. Harmonising them means that all of our geo-spatial layers are in the same format: CRS, extent, spatial resolution, and shape. Why is this necessary? Short answer: so that each pixel with values of geo-spatial data describes the same location across all our geo-spatial datasets.

Long anwer: In order to predict poverty rates for upazilas, we need to know some basic statistics (mean, max, min, sd etc.) for the values of our geo-spatial variables at upazila level. For example: what is the mean population density of women and nightlights intensity in each upazila in Bangladesh? We would only be able to answer this question if our layers showing population density for women and nightlights were aligned: they had the same CRS (projection), extent (maximum and minimum values of geographical coordinates), spatial resolution (size of pixels), and shape of Bangladesh. That is, we need comparable pixels with data values that can be assigned to upazilas.

@PJ - *I really think some of the above could use some visuals. I can see you have the overarching steps in a visual below - but I also think that some of the reasons for why we need this can be visualised using graphs.*

Finally, we will need a shapefile that defines the boundaries of each administrative zone - so that we can assign each pixel with geo-spatial information to the right upazila.

Figure XX below demonstrates the process in six steps:

**Step 1**: Load all datasets. I have divided those into 6 categories:

1.  Base raster layer - this raster contains the target values of parameters, such as: CRS, extent, spatial resolution, and shape. We will be harmonising to this base layer, i.e. we want all our other layers to look like this one.
2.  Shapefile with zonal boundaries - we need this to define the administrative boundaries of upazilas in Bangladesh.
3.  Nightlights intensity data
4.  Malaria Atlas data on accessibility
5.  Meta Demographic Maps
6.  WorldPop Harmonised data on Topography.

Datasets no. 3-5 will need harmonising, while the World Pop datasets are pre-harmonised and do not need any processing from us.

See Figure XX for an illustration.

**Step 2**: Set extent the same as the Base Layer. We want our layers to cover just Bangladesh (as opposed to, for example, the entirety of South-East Asia). Setting the extent will ensure that we are working within the same range of coordinates.

**Step 3**: Change the spatial resolution to that of the Base Layer (\~100m). This means all our data will be organised into pixels covering an area of 100m x 100m.

**Step 4**: (only for Meta Demographic Maps): recode missing values as 0s.

**Step 5**: Mask to obtain the "shape" of Bangladesh.

**Step 6**: Estimate zonal statistics (mean, max, min, sd etc.) for each geo-spatial dataset for each upazila.

![](images/daigram%20vol%203.png)

## 2. Theory - key concepts in Geo-spatial analysis

Let us start with the two most burning questions: What is geo-spatial data and what is a GIS?

**Geo-spatial data** in the simplest terms is any type of data referenced to a specific location on Earth. It consists broadly of two elements: **1) location data** (where things are) and **2) attributes data** (what things are like there). Location data allows us to position our data values adequately on the surface of the Earth. Attributes data describes any features of that specific location (temperature, elevation, road type, a bridge, demographic information and so on).

**Geographic Information System (GIS)** is a system that creates, manages, analyses, and maps data. GIS connects data to a map, integrating location data with all types of descriptive information.

### 2.1. Geo-spatial data sources

#### Traditional surveys

Recall that geo-spatial data is simply a set of data values that are indexed to a geographical location on Earth. In that sense, traditional census or surveys are sources of geo-spatial data as long as they contain information on administrative units, GPS location, or any other geographical identifiers. Data coming from traditional surveys can even be more precise, for instance if we are interested in measuring a building and its properties.

#### Remote sensing

Increasingly, **remote sensing** data collection is turning out to be a very efficient way of collection geo-spatial data. This includes satellite imagery and aircraft sensors that orbit the Earth and collect data continuously, at a high spatial resolution, and globally, including places that would be hard to reach with a survey (say the middle of the ocean or conflict-affected areas). In fact, due to the advantages of remote sensing data collection - space is becoming a crowded place, and increasingly more so in the last couple of years. Figure X below shows the time trend of the annual number of objects launched into space, which explodes upwards after around 2015. As of 2022, there were 6,905 active satellites in space. In this workflow, we will be relying heavily on remote sensing data, but let us briefly consider other sources of geo-spatial information.

![](images/Objects%20launched%20into%20space.png){width="573"}

Source: [Our World in Data](https://ourworldindata.org/space-exploration-satellites)

#### Crowdsourcing

For all the growing interest in remote sensing data collection, it is worth acknowledging that highly valuable geo-spatial information often comes from humans on the ground! Crowdsourcing geo-spatial data is often done via informal social networks and does not require any formal training in geo-spatial technologies. Perhaps one of the most notable examples is [Open Street Map](https://www.openstreetmap.org/about) (OSM), a community of mappers with local knowledge that produce open source maps. The [Humanitarian team at OSM](https://www.hotosm.org/) produces community-developed maps in areas such as Disasters & Climate Resilience or Public Health - [for example](https://www.hotosm.org/projects/integrating-openstreetmap-data-into-caribbean-disaster-response-efforts-geocris/), OSM is working with the World Bank and the Caribbean Disaster Emergency Management Agency (CDEMA) to integrate their maps into official disaster risk response.

#### Cell phone data

Basic services provided by cell phone operators are routed by satellites, providing location information at various levels of precision (from GPS to cell phone tower), together with Call Detail Records (CDR). [Flowminder](https://www.flowminder.org/about-us) is an example of an organisation that leverages mobile operator data in development projects, for instance tracking population movements post the 2015 Nepal Earthquake.

#### Online data

Online data is broad and includes all data shared online purposefully, including Wikipedia, social media, online articles etc. Location and mobility tracking via social media can be another source of geographical information. App-based mobile phone location data is actually often of higher spatial resolution than that coming from cell phone data operators and is increasingly being used in mobility studies.

### 2.2. Data structures

I want to introduce the two most common spatial data types, which we will be working with in our poverty mapping exercise, together with their attributes.

::: callout-important
## Important

An important note on the naming conventions and structures of geographical coordinates in geo-spatial data. A set of coordinates will typically be expressed as **(x,y), where x relates to longitude and y to latitude**. Latitude has a range (-90, 90), while longitude has a range (-180, 180). Negative values of longitude refer to the Western hemisphere and positive values to the Eastern hemisphere. Similarly, negative values of latitude refer to the Southern hemisphere and positive values refer to the Northern hemisphere. Here is a diagram that can serve as a cheat sheet!

![](images/geographic-coordinate-system-l-01.jpg){width="428"}
:::

#### 2.2.1. Raster data

**Raster data** is a collection of cells (or pixels) organised into rows and columns (called a grid), defined by its location in space. The raster is the pattern of locations that together - as a grid - cover a geographic area. Cells have values that carry information about those locations, such as temperature or elevation. Values can be continuous (e.g. a range of temperature values), or categorical (e.g. different categories of land use). If pixels sound familiar, it's because they are - in fact, this is how every digital image is represented. The only difference is that a raster additionally includes spatial information that connects data from the image to a particular location. The location is defined by the following attributes (these are important attributes to inspect when you first open your raster):

-   Extent

-   Cell size (spatial resolution or size of each pixel)

-   The number of rows and columns

-   Coordinate Reference System (CRS)

    To illustrate this concept, below is a raster image showing the elevation of Harvard Forest. Each pixel has values of elevation assigned, which range from 0-9. Here, the spatial resolution is 1m x 1m, which means each pixel has an area of 1m by 1m (this is really granular!).

    ![](images/Raster%20image.png){width="574"}

Source: [National Ecological Observatory Network](https://www.neonscience.org/)

#### 2.2.2. Vector data

**Vector data** represents structures on the Earth's surface that are defined by discrete locations (x,y) called vertices. Depending on how the vertices (x,y) are oragnised, we can divide vector data types into: **points, lines, and polygons**. You could think of: locations of hospitals, road networks, administrative boundaries.

![](images/Geometry%20vector.png){width="568"}

Source: [National Ecological Observatory Network](https://www.neonscience.org/)

We could add another category to this, which is the **multipolygon**. As the name suggests, a multipolygon is a collection of polygons. For instance, if we think about a polygon representing a lake or a forest cover, a multipolygon could depict an area of a national park that consists of a forest cover *and* two lakes - such as in the image below. In this particular example our polygons are *nested* within each other, but they could very well be just next to each other and not touching (such as two buildings on either side of a street).

![](images/Multipolygon.png)

To illustrate the concept further, below is a vector data set with lines showing the road network in Ghana, developed through Open Street Maps Ghana, downloadable via the Humanitarian Data Exchange website. Each line is determined by its vertices (think of them as the (x,y) points shown in the image above) and indexed geographically in space.

![](images/Ghana%20road%20network.png){width="648"}

Source: [Humanitarian Data Exchange](https://data.humdata.org/dataset/hotosm_gha_roads)

Vector data comes in many formats, but we will be working with the (very common) Shapefile format, which has a .shp extension. More specifically, we will download a shapefile containing multipolygons that define administrative boundaries of upazilas (admin level 3) in Bangladesh. A shapefile stores the coordinates of vertices, as well as other metadata:

-   Extent

-   Object type - whether the shapefile contains points, lines, or polygons.

-   Coordinate Reference System (CRS)

-   Other attributes - e.g. in our case the name of the upazila, its administrative code, area covered etc.

::: callout-tip
## Quick quiz alert

Now, let's do a super quick quiz to check our intuitition, which goes as follows: I will present to you a few examples of geo-spatial datasets and invite you to either type in the chat or shout out directly whether this is a vector or a raster dataset.
:::

### 2.3. Layering geo-spatial data sets

Now that you are familiar with the basic types of geo-spatial datasets, we can start thinking about performing some simple mapping. Typically, rather than viewing each layer of data (such as your road network, elevation, temperature etc.) individually, it is valuable to overlay them on top of each other, so that we can view multiple geo-spatial layers at the same time. The diagram in Figure XX below demonstrates this concept: while we can inspect data on streets, buildings, and vegetation individually, a much more interesting way to look at these layers is by viewing the simultaneously, as in the "Integrated data" portion of the diagram at the very bottom.

![](images/Data%20layers.jpg){width="555"}

Source: [National Geographic](https://education.nationalgeographic.org/resource/geographic-information-system-gis/)

::: callout-caution
## Caution

Make sure that the CRS is the same for all of your objects before layering. The most standard CRS is the World Geodetic System 1984 (WGS 84), but you may encounter other ones when working with spatial data for specific regions of the world.
:::

Now, let's see an example of visualising multiple raster layers from our project work under [Data & Evidence to End Extreme Poverty (DEEP)](https://povertyevidence.org/). In this case, we wanted to map the spatial distribution of climatic risks in Nigeria, looking at droughts and floods. Figure XX below shows the worst drought (defined as Standardized Precipitation Evapotranspiration Index or SPEI below -1.5) and all flooded areas in the period 2010-2020 in Nigeria, on top of an Open Street Map background. Drought is captured by a continuous variable for a range of values of SPEI (large pixels from yellow to red), while flood is a categorical variable (i.e. flood or no flood), with flooded areas shown in blue. We can see that the extent of these layers differs, i.e. drought is only defined within Nigeria (including bordering grid cells), while flood data extends beyond the borders of Nigeria and considers an entire river system, which is not necessarily bound by national borders.

![Rasters showing all floods and the worst droughts in Nigeria between 2010-2020](images/Nigeria%20flood%20drought%20.png)

The two layers (drought and flood) also have different spatial resolutions. Here, drought data is of a lower spatial resolution, i.e. it comes up as big pixels of around 0.5 degrees (or \~30km x 30km), while flood data is extremely high resolution, with each pixel the size of \~250m x 250m, coming from the NASA MODIS satellite. Figure XX below shows a zoom into two grid cells of drought data, which contain numerous grid cells with flood data. The higher the spatial resolution, the smaller your grid cells!

![Zoom into pixels showing floods and droughts in Nigeria](images/Pixel%20zoom%20in%20.png)

### 2.4. Raster manipulation for poverty modelling

The previous section introduced the concept of working with multiple layers of geo-spatial information. In our example, the two layers, despite being displayed on top of each other, had very different attributes: spatial resolution, cell sizes, extent . However, there are cases, as in the poverty modelling exercise we are about to do, where we want to "harmonise" our raster objects, i.e. make sure all the attributes (CRS, extent, spatial resolution, cell size) are the same when rasters are overlaid on top of each other.

Let's consider our problem at hand again: **We want to compute poverty statistics at administrative level 3 (upazila) level in Bangladesh.** We will do so by using statistical modelling and a number of remote sensed covariates as our predictors. For this, we need some basic statistics (mean, min, max, standard deviation, count etc.) that describe geo-spatial information for each upazila. For example, what is the mean Travel Time to Healthcare Facilities (in minutes) in the upazila of Abador? Or, what is the median nightlights intensity in that upazila? While R is smart enough to compute those statistics for us automatically, we need to provide it with a nicely harmonised set of rasters. We will use a "base layer", which contains all the attributes we are interested in, and harmonise all other remote sensing covariates to our base layer. That is: we will match the CRS, extent, spatial resolution, cell size of all our data to the one of the base layer.

With that in mind, I will introduce a few ways in which we will manipulate spatial data. This is a useful skill to have as we will soon discover that raw open source data can be a little messy and you may need to implement some of these steps to make sense of your data before any meaningful analysis.

1.  **Cropping -** changing the extent of our spatial objects, i.e. the maximum and minimum values of longitude and latitude that define the object, to that of the base layer.
2.  **Resampling** - changing the spatial resolution so that it matches that of the base layer.
3.  **Recoding missing values** - (only if appropriate) changing missing values to a specified value.

::: callout-caution
## Caution

A note of caution on missing values in geo-spatial data. As with every data type, the treatment of missing values can be a source of philosophical discussions and should always be determined depending on the context and the specific problem at hand. The user should always ask themselves: can a missing value be interpreted as a zero or do we simply not have enough information to conclude? If it is a zero, missing values can be recoded accordingly. Be careful with this. If in doubt, always consult an expert.
:::

4.  **Masking** - cutting the exact "shape" of a raster layer. This is different from cropping, which would leave us with a chunky box that contains Bangladesh, but also some areas between the maximum and minimum values that fall outside of Bangladesh. We can use the mask function to examine only the country boundaries.

These concepts may seem a little abstract for now, so let us consider visually what each of these processes will entail. Let's take the example of processing geo-spatial data on nightlights intensity for Bangladesh - a process we will be implementing in Step 5 of Section 3. Night lights intensity has been widely used in literature as a proxy for income and will be one of our predictors of poverty. Figure XX shows a TIFF file with the aerial shot of nightlights intensity in South-East Asia as seen from space. We can see that urban areas and areas with higher density of economic activity appear more luminous. We will use the functions described above to manipulate this TIFF file and transform it into a raster layer for Bangladesh with codified values of radiance per pixel (this is a measure of nightlights intensity).

![](images/Nightlights%20data%20-02.png){width="494"}

Figure XX below shows the processing of data on nightlights intensity. The left hand side is what you will see if you simply load the above TIFF file into your R environment, turn it into a raster, and plot - in other words, a massive block of dark blue over Asia. The original raster shows values of nightlights intensity for the entire South East Asia region at the original spatial resolution of around \~450m.

First, we are interested in analysing just Bangladesh, rather than the entirely of South East Asia (not least because TIFF files can be quite heavy and maintaining unnecessarily large raster objects slows down a lot of the computation). We will "crop" our raster object to the Bangladesh base layer, i.e. we will change the extent to that of Bangladesh. Now, our raster object changed dimensions. It is defined by the maximum and minimum values of longitude and latitude in Bangladesh.

We also want to view the "shape" of Bangladesh, rather than a rectangular box bound by its extent. For this purpose, we "mask" our raster layer with nightlights to the base layer - and we achieve a nicely formatted raster that resembles Bangladesh!

Just a note that along this process, we also resample the raster, i.e. change the spatial resolution to \~100m to match that of our base layer. Going from a lower to a higher spatial resolution is called disaggregation. If we zoomed into the image, we would see more of smaller pixels appear than previously. However, since the diagram is meant to demonstrate how we change the geographical coverage, it is quite zoomed out and luminous pixels in yellow do not appear immediately visible. Also, given that we are working with a really high spatial resolution - we would need to zoom *a lot*, possibly to the level of individual buildings, in order to see the change resulting from resampling (hence I do not show it in the diagram).

![](images/Nightlights%20processing%203.png){width="720"}

### 2.5. Spatial Data in R

Given that our entire workflow will be carried out in R to make it accessible, open source, and user-friendly - we will introduce some common spatial objects in R. There are numerous packages that allow you to work with spatial data. In fact, packages are very often retired and new ones are created to replace them. We will be relying mostly on:

1.  The [`sp`](https://www.rdocumentation.org/packages/sp/versions/2.0-0) and [`sf`](https://www.rdocumentation.org/packages/sf/versions/1.0-14) packages for vector data
2.  The [`raster`](https://www.rdocumentation.org/packages/raster/versions/3.6-23/topics/raster-package) package for raster data
3.  The [`rgdal`](https://www.rdocumentation.org/packages/rgdal/versions/1.6-7) package for manipulation of geo-spatial data using the GDAL library.

#### 2.5.1. Vector data with `sp` package

Recently spatial data in R has been standardised using the base package `sp`. Although a newer version of this package exists, called `sf` , it is useful to understand basic data structures under `sp`. Let's have a look at the spatial data classes by running the chunk below:

```{r sp package}
library(sp)
getClass("Spatial")
```

A basic `sp` object will have two "slots": a bounding box `bbox` containing a matrix of longitude and latitude values (or, using the language we introduced earlier, the extent of an object) and `proj4string`, which is the CRS. These parameters will help us locate our object in space. **The basis of all the spatial objects is a matrix or data frame with latitude and longitude values - this is what we need to make our existing objects in R "spatial".** From the output above, we can also see that objects build on each other. This is illustrated in Figure XX below. For example, taking our matrix or data frame with latitude and longitude, we can create a `SpatialPoints` object, which would simply be a collection of spatial points. We can build on this and create a `SpatialPointsDataFram`e by adding a data frame with attributes associated with these points. The same can be done for lines and polygons.

![](images/spatial%20objects%20in%20R.png)

#### 2.5.2 Vector data with `sf` package

The `sf` package implements [Simple Features](https://en.wikipedia.org/wiki/Simple_Features), which is a set of standards that specify storage and model of geographic features used in geographic information systems. It is famous enough to have its [Wikipedia page](https://en.wikipedia.org/wiki/Simple_Features) and is endorsed by the Open Geospatial Consortium. It works only with vector data and does not accommodate rasters. Common geometry types are shown in Figure XX below. Among the primitive ones, we have `Points`, `LineStrings`, and `Polygons` (the same as simple vector geometry types discussed in Section 2.2.) The `sf` package allows for multipart geometries, essentially a collection of primitive geometries, such as: `MultiPoints`, `MultiLineString`, and `MultiPolygons`. The coordinates in brackets specify each vertex that defines the geometry.

![](images/sf%20package%20primitive%20geometries.png)

![](images/multipart%20geometries.png)

Source: [Wikipedia](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry)

`sf` objects are organised into three elements. Figure XX shows an example of an `sf` object, which displays 100 features (or rows in our data) and 6 fields (columns in our data). The key elements that make up this object are:

1.  `sf`, an individual row or record in our data with values for each column (green box).
2.  `sfc,` the simple feature geometry list-column (red box). This column indicates that our object is indeed 'spatial', as opposed to being a simple data frame. It contains the geometry type for each row in our data (see geometry types we learned in the figure above: they could be lines, points, multipoints etc.). In this case, we are working with multipolygons (which could be for instance administrative boundaries).
3.  `sfg`, the simple feature geometry (blue box). It contains the actual geographical coordinates - or (x,y) points, that make up our geometry. In our case, they will show all (x,y) points that are vertices of multipolygon structures.

![](images/multipolygon%20feature%20.png)

Source: [CRAN project, sf vignette](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html)

#### 2.5.3. Raster data with `raster` package

This is the main geo-spatial data type we will be working with for our poverty modelling exercise. The `raster` package provides functions that allow us to read and write any raster object and perform raster operations such as those described in Section 2.4. (resampling, masking, cropping, recoding NAs etc.). It supports three classes of objects: `RasterLayer` (a single raster), `RasterStack`(multiple rasters stacked on top of each other that can still be viewed as individual files), `RasterBrick` (similar to `RasterStack` but in a single file).

To explore the structure of raster data let us create en empty (no data values) raster from scratch by running the code below. We will specify the following parameters:

-   Number of rows and columns.

-   Extent (minimum and maximum values of longitude and latitude).

```{r empty raster}
library(raster)
r <- raster(ncol=10, nrow = 10, xmx=-116,xmn=-126,ymn=42,ymx=46)
r
```

Note that just specifying the number of rows and columns and extent was sufficient to create a `RasterLayer` object. The number of rows and columns defined our spatial resolution of the raster. In addition, the standard CRS `+proj=longlat +datum=WGS84 +no_defs` was automatically assigned. At the moment, our raster is empty, i.e. it is simply a grid with no data values. Let us generate some data to populate the raster by assigning random values from the unit distribution to each grid cell.

```{r fill raster}
r[] <- runif(n=ncell(r))
r
```

Now the summary of our RasterLayer object includes minimum and maximum values of our randomly generated data. Let's visualise this!

```{r plot raster}
plot(r)
```

### 2.6 Homework

1.  [Visualising Spatial Data in R at DataCamp](https://app.datacamp.com/learn/courses/visualizing-geospatial-data-in-r) OR [Spatial Objects in R](https://cengel.github.io/rspatial/2_spDataTypes.nb.html) (optional for those that wish to become familiar with spatial objects in R and creating those from scratch).
2.  **Download the "Geo-spatial Bangladesh data" zip file, unzip it, and save it in your working directory. Your folder structure after unzipping should look like this:**

![](images/folder%20structure%20vol%202-01.png)

I would recommend not rearranging the files across folders. Keeping this structure will make it easy to run your code in Section 3 as you will only need to replace the base Working Directory with your own and everything else should run from there!

::: callout-warning
## Warning

Just a heads up that the files we will be working with are very large. The .zip file itself is is 5GB, while all the files after unzipping will take up around 34GB of space. Please make sure you have sufficient memory on your machine (a minimum of 34 GB will be required). If you are using an online storage solution, you might want to temporarily offload other files you are not currently using to the cloud.
:::

## 3. Applying this

In Section 2 we covered key concepts in geo-spatial analysis, including the sources of such data, most common structures, data manipulation and types of spatial objects you might encounter in R. In this section we are going to apply this to a practical problem: **we will prepare the geo-spatial covariates (on accessibility, nightlights, topograhy, and demography) that will be used for poverty estimation in Section B**. Recall that there are two main types of geo-spatial data: vector and raster. We will primarily be working with raster data and performing what we will call a "harmonisation" - that is, bringing all the rasters to the same extent, shape, spatial resolution, and projection as the base raster layer. Let's begin with exploring the data sources we will use in this exercise.

### Data sources - Bangladesh

We will be working with the following data sets:

1.  **Base layer**: WorldPop Administrative Level 0 mastergrid base layer for Bangladesh at \~100m spatial resolution. This mastergrid defines the administrative boundaries of the country, to which we will be harmonising all other geo-spatial datasets, i.e. matching the spatial resolution, CRS, extent, and shape.

2.  **Shape file**: Subnational Administrative Boundaries for Bangladesh from the Humanitarian Data Exchange. This file contains the administrative boundaries of the zones for which we will be estimating poverty. In our case, this is done at administrative level 3 (upazila) in Bangladesh.

3.  **Malaria Atlas data on accessibilit**y, including: travel time to cities and healthcare facilities.

4.  **Demographic maps from Meta** at \~30m spatial resolution raster showing population density of different demographic groups (men, women, youth, elderly etc.).

5.  **Nightlights data** at \~450m spatial resolution with almost global coverage.

6.  **World Pop** Pre-harmonised geo-spatial datasets: available for all countries. The datasets includes information on topography (this relates to land cover classes - for instance, waterways, forests, mountains, plains ets.), slope (measure of change in elevation), and Open Street Map's data on distance to closest road, waterway*.* Note that this data has already been harmonised to our base layer and hence will not require any processing in this script.

The below tables provide details on these data sets.

### Base layers

| Geo-spatial file name \| Description \| Year \| Link to download \|

\|:-----------------\|:-----------------\|:-----------------\|:------------------\|

\|:----------------\|:----------------\|:----------------\|:----------------------\|

| bgd_level0_100m_2000_2020.tif \| WorldPop Administrative Level 0 mastergrid base layer for Bangladesh \| 2020 \| [World Pop Hub - National boundaries](https://www.worldpop.org/geodata/summary?id=24282) \|
| bgd_admbnda_adm3_bbs_20201113.shp \| Administrative Level 3 (Upazila) Units in Bangladesh \| 2015 \| [Bangladesh - Subnational Administrative Boundaries - Humanitarian Data Exchange](https://data.humdata.org/dataset/cod-ab-bgd) \|

### Geo-spatial data sets to be harmonised

|                                                                             \| \| \| \|

\|------------------\|------------------\|:-----------------\|-------------------\|

\|----------------\|----------------\|:---------------\|------------------------\|

| **Geo-spatial file name** \| Description \| Year \| Link to download \|
| SVDNB_npp_20150101-20151231_75N060E_vcm-orm_v10_c201701311200.avg_rade9.tif \| VIIRS night-time lights (global) \| 2015 \| [Earth Observation Group](https://eogdata.mines.edu/products/vnl/#v1) \|
| 2015_accessibility_to_cities_v1.0.tif \| MAP travel time to high-density urban centres (global) \| 2015 \| [Malaria Atlas - Accessibility to Cities](https://data.malariaatlas.org/maps?layers=Accessibility:201501_Global_Travel_Time_to_Cities,Malaria:202206_Global_Pf_Parasite_Rate) \|
| 2020_motorized_travel_time_to_healthcare.tif \| MAP motorised only travel time to healthcare facilities (global) \| 2019 \| [Malaria Atlas - Motorised Time Travel to Healthcare](https://data.malariaatlas.org/maps?layers=Accessibility:202001_Global_Motorized_Travel_Time_to_Healthcare,Malaria:202206_Global_Pf_Parasite_Rate&extent=-11815912.856289707,-6356003.33856192,28286163.259866484,14615055.359158086) \|
| 2020_walking_only_travel_time_to_healthcare.tif \| MAP walking only travel time to healthcare facilities (global) \| 2019 \| [Malaria Atlas - Walking Only Time Travel to Healthcare](https://data.malariaatlas.org/maps?layers=Accessibility:202001_Global_Walking_Only_Travel_Time_To_Healthcare,Malaria:202206_Global_Pf_Parasite_Rate&extent=-11815912.856289707,-6356003.33856192,28286163.259866484,14615055.359158086) \|
| bgd_general_2020.tif \| Meta (Facebook) Population density -- Overall; BGD \| 2020 \| [Humanitarian Data Exchange - Demographic Maps from Meta](https://data.humdata.org/dataset/bangladesh-high-resolution-population-density-maps-demographic-estimates) \|
| bgd_men_2020.tif \| Meta (Facebook) Population density -- Men; BGD \| 2020 \| \|
| bgd_elderly_60_plus_2020.tif \| Meta (Facebook) Population density -- Elderly (aged 60+); BGD \| 2020 \| \|
| bgd_women_of_reproductive_age_15_49_2020.tif \| Meta (Facebook) Population density -- Women of reproductive age (15-49); BGD \| 2020 \| \|
| bgd_women_2020.tif \| Meta (Facebook) Population density -- Women; BGD \| 2020 \| \|
| bgd_youth_15_24_2020.tif \| Meta (Facebook) Population density -- Youths aged 15-24; BGD for 2020 \| 2020 \| \|
| bgd_children_under_five_2020.tif \| Meta (Facebook) Population density -- Children under 5; BGD \| 2020 \| \|

### WorldPop harmonised geo-spatial data sets

|                                           |                                                                         |           |                                                                |
|------------------|-------------------|------------------|------------------|
| **Geo-spatial File Name**                 | **Description**                                                         | **Year**  | **Link to Download**                                           |
| bgd_srtm_topo_100m.tif                    | SRTM topography in BGD                                                  | 2000      | [World Pop](https://www.worldpop.org/project/categories?id=14) |
| bgd_srtm_slope_100m.tif                   | SRTM slope (derivative of topography) in BGD                            | 2000      |                                                                |
| bgd_esaccilc_dst040_100m_2014.tif         | Distance to ESA-CCI-LC woody-tree area edges in BGD                     | 2014      |                                                                |
| bgd_esaccilc_dst140_100m_2014.tif         | Distance to ESA-CCI-LC herbaceous area edges in BGD                     | 2014      |                                                                |
| bgd_osm_dst_road_100m_2016.tif            | Distance to OSM major roads in BGD                                      | 2016      |                                                                |
| bgd_esaccilc_dst150_100m_2014.tif         | Distance to ESA-CCI-LC sparse vegetation area edges in BGD              | 2014      |                                                                |
| bgd_osm_dst_roadintersec_100m_2016.tif    | Distance to OSM major road intersections in BGD                         | 2016      |                                                                |
| bgd_esaccilc_dst160_100m_2014.tif         | Distance to ESA-CCI-LC aquatic vegetation area edges in BGD             | 2014      |                                                                |
| bgd_osm_dst_waterway_100m_2016.tif        | Distance to OSM waterways in BGD                                        | 2016      |                                                                |
| bgd_esaccilc_dst190_100m_2014.tif         | Distance to ESA-CCI-LC artificial surface edges in BGD                  | 2014      |                                                                |
| bgd_esaccilc_dst011_100m_2014.tif         | Distance to ESA-CCI-LC cultivated area edges in BGD                     | 2014      |                                                                |
| bgd_esaccilc_dst200_100m_2014.tif         | Distance to ESA-CCI-LC bare area edges in BGD                           | 2014      |                                                                |
| bgd_esaccilc_dst_water_100m_2000_2012.tif | Distance to ESA-CCI-LC inland water in BGD                              | 2012      |                                                                |
| bgd_wdpa_dst_cat1_100m_2014.tif           | Distance to IUCN strict nature reserve and wilderness area edges in BGD | 2014      |                                                                |
| bgd_esaccilc_dst130_100m_2014.tif         | Distance to ESA-CCI-LC shrub area edges in BGD                          | 2014      |                                                                |
| bgd_dst_coastline_100m_2000_2020.tif      | Distance to open water coastline in BGD                                 | 2000-2020 |                                                                |

### Libraries

Throughout this workflow, we will be using functions from the packages shown in the chunk below. Most of them allow for manipulation of spatial objects in R, others are used to enhance our data visualisations. All the below packages need to be installed and loaded in order for the workflow to run smoothly. If you don't already have some of the below packages installed, you can do so by running: install.packages("your.package.name"). After that, you will be able to load the package using library("the.package.you.just.installed"). See below the dependencies we will need for this workflow:

```{r libraries}
#| echo: true
#| output: false

library("tiff")           # opening geoTIFF files
library("raster")         # raster manipulation
library("sf")             # Simple Features vector data
library("ggplot2")        # plots
library("RColorBrewer")   # funky graph colors 
library("haven")          # opening stata files
library("gsubfn")         # string operations
library("viridis")        # colour palette for data visualisations
library("rgdal")          # RGDAL functions
library("sp")             # Spatial Points vector data
library("plyr")           # Data wrangling
library("dplyr")          # Data wrangling
library("readr")          # read rectangular data
library("leaflet")        # Interactive maps
library("htmlwidgets")    # HTML widgets
library("scales")         # Nice scales on graphs
library("exactextractr")  # Automated production of zonal statistics
library ("cividis")       # Colour palette for data visualisations
```

### Step 1 - Load all data sets

Our first step will simply be loading all the datasets that we will work with into our environment. The only step that the user needs to complete here is to specify the working directory where the .zip file "Geo-spatial Bangladesh data.zip" is saved (and unzipped). As a reminder, your folder structure should look like this after unzipping:

![](images/folder%20structure%20vol%202-02.png)

If this looks right, you are ready to run your first chunk of code below. This will load all the data we will need for this exercise into your environment. Whenever a .tif extension is used, we will import the file and convert it to a raster (recall this is the main format we will be working with). Some folders will have multiple files saved inside them (for example, there are three datasets from the Malaria Atlas Project). In this case, the code will pull all the files that have a .tif extension and save them in a list - so no need to import them individually!

```{r load layers}
#| echo: true
#| output: false

# Set working directory - this is the base folder where we keep all the sub-folders with geo-spatial data organised by source. 
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/EMDI SAE/Geo-spatial Bangladesh data/")

# 1. Base layer #

# Load the WordPop Administrative Level 0 mastergrid base layer for Bangladesh 
base.layer <- 'Base layers/bgd_level0_100m_2000_2020.tif'
base.layer.raster=raster(base.layer)

# 2. Shapefile with boundaries of zones #

# Load the Shapefile with administrative level 3 (upazila in Bangladesh)
shapefile.zonal <- readOGR("Base layers/admin 3 level/bgd_admbnda_adm3_bbs_20201113.shp")

# Load the shapefile in a format that's nicer for plotting 
shapefile.zone.plot <- st_read("Base layers/admin 3 level/bgd_admbnda_adm3_bbs_20201113.shp")

# 3. Malaria Atlas accessibility datasets #

#Make a list of all TIFF files from the Malaria Atlas Project and import those into a list 
matlas.list <- list.files(path="Travel time - Malaria Atlas Project", pattern =".tiff", full.names=TRUE)

# Turn all files into a raster and make a list of rasters
matlas.rasters <- lapply(matlas.list, raster)

# 4. Demographic maps from Meta #

# Pull all TIFF files from Meta with demographic maps
fb.list <- list.files(path="Demographic maps Facebook/", pattern =".tif", full.names=TRUE)

# Turn all files into a raster and make a list of rasters
fb.rasters <- lapply(fb.list, raster)

# 5. Nightlights Data #

# Import geoTIFF file on nightlights intensity
nightlights <- 'Nightlights data/SVDNB_npp_20150101-20151231_75N060E_vcm-orm_v10_c201701311200.avg_rade9.tif'

# Turn into a raster 
nightlights.raster = raster(nightlights)

# 6. Pre-harmonised World Pop datasets #

# Make a list of all TIFF files from WorldPop and import those into a list 
worldpop.list <- list.files(path="World Pop Harmonised Datasets", pattern =".tif", full.names=TRUE)

# Turn all files into a raster and make a list of rasters
worldpop.rasters <- lapply(worldpop.list, raster)


```

You should now be able to view all the data in your environment. Most objects are of the class `RasterLayer`, which we discussed in Section 2.5.3. Let's have a closer look at how these objects are structured in practice and the information they contain by inspecting our base layer.

#### Check the attributes of the base layer

As we will be harmonising all other datasets to our base layer, it is important to understand its attributes. Simply typing the name of our base layer raster object in the console will provide information on: class, dimensions, resolution, extent, crs, source, and names. At the end of the workflow, all of our geo-spatial covariates will have the same spatial resolution, CRS, extent, and shape as our base layer.

```{r plot base raster}
# Check attributes associated with the base layer 
base.layer.raster

# Plot raster
plot(base.layer.raster, box=F, axes=F,legend=F, main = "Bangladesh base layer")
```

The output is telling us that our object of interest is a `RasterLayer`, with the following characteristics:

-   7272 rows, 5596 columns, and 40694112 cells altogether - these are the dimensions of our grid.

-   A spatial resolution of 0.0008333333 by 0.0008333333 (in degrees). This is equivalent to \~ 100m spatial resolution and defines the size of each cell in the raster.

-   Extent, i.e. the maximum and minimum values of longitude and latitude which define Bangladesh.

-   The WGS 1984 datum (EPSG: 4326) geographical projection - this is the most standard global projection.

-   It came from the GeoTIFF file named "bgd_level0_100m_2000_2020.tif" in our working directory and is named after that file.

We also produced our first map of Bangladesh! This is the raster layer we will be harmonising all our other data to. Note that since this layer simply gives us the administrative national boundaries of Bangladesh, it does not contain any data values. While most of our objects are raster data, we will use one vector dataset: a shapefile containing the boundaries of administrative level 3 (upazilas) in Bangladesh. There are multiple ways of storing vector data in R , as discusses in Section 2.5 - let's have a look.

#### Inspect the attributes of the shapefile with administrative level 3

Apart from the raster file defining the national boundaries of Bangladesh, we are interested to know the boundaries of the administrative zones that we will estimate poverty for. In our case, this is administrative level 3, or upazila, in Bangladesh. This information is provided by a shapefile, which is a *vector* dataset - and has different properties than raster data.

As before, we will start by simply typing the name of our spatial object into the console. This will provide information on: geometry, dimensions, extent, CRS, as well as give us a preview into the features. To demonstrate this, I saved our shapefile in two formats: 1) an `sf` object with `multipolygon` geometry, and 2) a `sp` object called `SpatialPolygonsDataFrame`.

```{r plot shapefile}
# Inspect the sf multipolygon object with admin 3 boundaries
shapefile.zone.plot

# Inspect the sp SpatialPolygonsDataFrame object with admin 3 boundaries
shapefile.zonal

# Plot the shape file - this shows the zones for which we will estimate poverty
ggplot() + 
  geom_sf(data = shapefile.zone.plot, size = 3, color = "black", fill = "cyan1") + 
  ggtitle("Administrative Level 3 Bangladesh") + 
  coord_sf() + theme_minimal()
```

The output gives us the following information about the shape file:

-   It has 544 features (upazilas) and 16 fields (variables) with information defining the features.

-   The geometry type is multipolygon. Each multipolygon is a collection of spatial points that define an upazila.

-   Bounding box shows the extent, i.e. maximum and minimum values of longitude and latitude that define Bangladesh.

-   As in the case of our base layer, it uses the WGS 1984 datum (EPSG: 4326) geographical projection.

-   A preview of the first 10 features tells us that for each multipolygon we have information on: administrative codes for the upazila, English name of the upazila, area covered by the shape, and codes as well as names for other administrative levels.

The plot shows a visualisation of the boundaries of each upazila in Bangladesh. They are essentially a collection of multipolygons (i.e. each upazila is bound by a multipolygon object). We will be estimating poverty for each upazila. This illustrates our issue quite nicely - it would be prohibitively costly for a survey like the DHS to travel to each upazila and obtain a sufficient sample size to generate reliable poverty statistics. By using open-source high-resolution remote sensed geo-spatial data, we can model poverty for each zone, including out-of-sample areas, i.e. areas where the DHS did not reach.

We now turn to Steps 2-5, which will process our raster layers of geo-spatial information.

### Steps 2-5 - Write functions for automated processing of geo-spatial covariates

Steps 2-5 all refer to the harmonisation of raster layers that contain geo-spatial covariates. The goal is for all our datasets (accessibility, demographic maps, night lights) to have the same: extent, spatial resolution, and shape as the base layer. In some cases, we may need to additionally address missing values. A detailed description of Steps 2-5 can be found in Section 1.

The reason that all steps are grouped together into one section is that all of those will be performed in a single automated function. We will write two functions, one for accessibility and nightlights data and the second one for demographic maps from Facebook. The only difference between them is the algorithm used for changing the spatial resolution of rasters.

::: callout-caution
## Caution

There are many algorithms that can be used for changing the spatial resolution of a raster, depending on the direction of resampling and types of geo-spatial information represented. We have two cases in our guide:

1.  Accessibility (\~250m resolution) and nightlights data (\~450 m resolution), which are at a coarser spatial resolution than our base layer (\~100m resolution), meaning they have fewer, larger grid cells. Moving from a lower to a higher spatial resolution is called disaggregation, as we see more smaller, or 'disaggregated' grid cells in the data after resampling. In this case, we can use the **nearest neighbour interpolation** method, where each new small pixel is assigned a data value of the older bigger pixel that is closest to it in terms of coordinate location. So, only the data value from *one* closest grid cell is transferred, and the values from other cells nearby are discarded.
2.  Demographic maps from Meta (\~30m resolution), which are at a higher spatial resolution than our base layer (\~100m resolution). This means our demographic data has more smaller pixels than the base layer. In this case, we have to "aggregate" a few smaller pixels into a bigger one to match the spatial resolution of our base layer. Given the nature of our data - where each pixel represents the number of people living there - we cannot simply take the value of *one* closest pixel, as is the case in the neareast neighbour interpolation, because we would discard information on all the other people living in neighbouring cells and underestimate the real population density. Instead, we have to use the **"sum" algorithm**, which sums up values from all smaller cells that fall within the bigger one (rather than just taking the value from one of them). This way, we will ensure that no people are 'lost' when aggregating. **In general, the use of the "sum" algorithm is the appropriate method when aggregating (changing to a lower spatial resolution) demographic raster data.**
:::

To automate the processing, we will write the first function for processing geo-spatial covariates and call it `geo_process`. We will use it for the Malaria Atlas accessibility and Nightlights data. It automatically executes the following steps:

1.  Crop your raster to the base layer so that they have the same extent.
2.  Change the spatial resolution of your raster to match that of the base layer using the nearest neighbor interpolation algorithm.
3.  If appropriate, recode missing values to a specified value.
4.  Mask the raster so that it has the same shape as your base layer.

The function requires the following inputs:

-   `aux.raster` - this is the raster that you are trying to transform/ harmonise to the base raster layer.

-   `base.raster` - your base raster layer that has all the desired attributes. This is the layer we will be harmonising to.

-   `label` - the name that you wish to assign to the transformed raster.

-   `value` - (if appropriate) the value to which you want to recode NA (missing) values to.

This process will output your transformed raster, ready to be used in the calculation of zonal statistics.

Running the chunk below will create the first function `geo_process`, which will show up under "Functions" in you environment.

```{r function ngb}
# Function for automating the processing of geo-spatial covariates for use in poverty modelling
geo_process <- function(aux.raster, base.raster, label, value) {

    # 1. Crop to the base layer so they have the same extent
    r <- crop(aux.raster, base.raster)

    # 2. Change resolution to the resolution of the base layer using nearest neighbour interpolation
    r <- resample(r, base.raster, method = "ngb") 
    
    # 3. Replace NA with a specified value (if appropriate)
    r[is.na(r[])] <- value  

    # 4. Select only the shape of Bangladesh as defined by the base layer
    r <- mask(x = r, mask = base.raster)
    
    # Assign a name to the raster
    names(r) <- label
    return(r)
    
}
```

The second function, called `geo_process2` is really similar, but applies the `sum` algorithm instead of nearest neighbour interpolation when changing the spatial resolution, which is the appropriate method when working with demographic maps from Meta that are of a higher resolution than our base layer. The demographic data from Meta are of a \~30m spatial resolution and require aggregation of the values in each cell when resampled to 100m. Using the nearest neighbour interpolation, we 'lose' people (because data is thrown away in the downsample). The'sum' algorithm instead provides a more accurate representation of population per each 100m grid cell.

Function 2 should be applied to all demographic maps from Meta and will execute the following steps:

1.  Obtain the ratio of spatial resolutions of your layer of interest (from Meta) and the base layer, save those as a `factor.ratio`.
2.  Crop your raster to the base layer so that they have the same extent.
3.  If appropriate, recode missing values to a specified value.
4.  Change the spatial resolution to match your base layer, on the basis of the ratio defined in Step 1. Note: we are going from a higher to a lower spatial resolution, hence the use of "aggregate".
5.  Mask the raster so that it has the same shape as your base layer.

Running the chunk below will create the second function `geo_process2` and it will show up under "Functions" in you environment.

```{r function sum}
# Function for automating the processing of geo-spatial covariates for use in poverty modelling
geo_process2 <- function(aux.raster, base.raster, label, value) {
  
    # 1. Get spatial resolutions of both rasters 
    res1 <- res(base.raster)
    res2 <- res(aux.raster)
    
    # Save as a factor of ratios 
    factor.ratio <- res1/res2
    factor.ratio <- factor.ratio[[1]]

    # 2. Crop to the base layer so they have the same extent
    r <- crop(aux.raster, base.raster)
    
    # 3. Replace NA with a specified value (if appropriate)
    r[is.na(r[])] <- value  

    # 4. Change resolution to the resolution of the base layer using the "sum"        algorithm
    r <- aggregate(r, fact=factor.ratio, fun=sum, na.rm=TRUE)

    # 5. Select only the shape of Bangladesh as defined by the base layer
    r <- mask(x = r, mask = base.raster)
    
    # Assign a name to the raster
    names(r) <- label
    return(r)
    
}
```

Now that we have two automated functions for processing geo-spatial data: `geo_process` and `geo_process2`, we can apply it to our raster datasets and create our first visualisations!

### Steps 2-5 - process accessibility data from the Malaria Atlas

In the chunk below, we will process accessibility data from the Malaria Atlas. Those are stored in a list of rasters named `matlas.rasters`. Simply typing the name of the object tells us more information about each raster in the list, including its class, dimensions, resolution, extent, crs etc. There are three datasets we will be using:

1.  Travel Time to Cities for Bangladesh in 2015
2.  Motorised Travel Time to Healthcae Facilities for Bangladesh in 2020
3.  Walking Only Travel Time to Healthcare Facilities for Bangladesh in 2020.

We will create a new list to store our processed datasets called `matlas.processed`, then use a loop to apply our `geo_process` function to all three Malaria Atlas datasets.

```{r matlas rasters}
# See that all the files have loaded
matlas.rasters

# Now apply this function to a list of rasters from the Malaria Atlas 

# Make a list for storing the processed rasters 
matlas.processed <- list()

# Store the length of the list of rasters
num_r <- 1:length(matlas.rasters)  

# Apply the geo-processing function to all rasters. Here there is no need to replace missing values as 0s (they are already coded as missing), so the parameter for value can stay at NA.
for (i in num_r) { 
matlas.processed[[i]] <- geo_process(aux.raster = matlas.rasters[[i]], base.raster = base.layer.raster, label = matlas.rasters[[i]]@data@names, value = NA)

}

# See the result
matlas.processed

# Compare to base layer
base.layer.raster

```

Viewing the ready `matlas.processed` objects, we can see that the dimensions as well as the extent have changed - they are now the same as our base layer.

Now let's visualise our results! In the code chunk below, I'll present three ways of visualising geo-spatial layers of information in R:

1.  A simple plot
2.  A high-quality plot image (same as point 1 but manually changing R's default plot resolution to improve quality)
3.  An interactive html widget using the `leaflet` package

```{r matlas visualisation}
# Three ways of visualising our data:

## 1. Simple plot ##

# Plot results - with no bounding box or axes, using the viridis colour palette, and adding a main plot title
plot(matlas.processed[[1]], box=F, axes=F, col=viridis(50), main = "Bangladesh - Travel Time to Cities")
plot(matlas.processed[[2]], box=F, axes=F, col=viridis(50), main = "Bangladesh - Motorised Travel Time to Healthcare Facilities")
plot(matlas.processed[[3]], box=F, axes=F, col=viridis(50), main = "Bangladesh - Walking-Only Travel Time to Healthcare Facilities")

## 2. A high-quality plot image ##

# Create a high quality image - manually changing the resolution of base R plots
png(filename="maps/BGD travel time city.png", res=400, width=2000, height=1500)
plot(matlas.processed[[1]], box=F, axes=F, col=viridis(50), main = "Bangladesh - Travel Time to Cities")
dev.off()

## 3. An interactive html widget ##

# First create a colour palette - let's go with viridis, and supply the values of our raster into domain
pal <- leaflet::colorNumeric(viridis_pal(option = "C")(2), domain = values(matlas.processed[[1]]), na.color = "transparent")

# Create an interactive leaflet map
map_access <- leaflet() %>%
 addProviderTiles(providers$CartoDB.Positron) %>%
  addRasterImage(matlas.processed[[1]],  colors = pal, opacity=0.9, project = FALSE)%>%
  addLegend(pal = pal, values = values(matlas.processed[[1]]), title = "Travel Time to Cities in Minutes")

# Let's have a look
map_access

# Save map as widget
saveWidget(map_access, file="html widgets/BGD_access_map.html")
```

Let's start with simple plots. The three plots show visualisations of travel time (in minutes) using the `viridis` colour palette. Coastal areas as well as western parts of the Chittagong division appear to be least connected to urban areas and healthcare facilities. While this is informative at first glance, a closer zoom in will typically appear quite pixelated. This is because R's default plot resolution is quite low for high-resolution geo-spatial data. We can change that manually.

In order to view this plot in higher quality, we need to change R's default plot resolution. The second option will export a higher-quality image of the map showing travel time to cities for Bangladesh and save it in the `maps` folder in your working directory. Unless you are cloning this repo from GitHub, in which case the folder will already be there, you may need to simply add a folder named `maps` in your working directory that you specified in Step 1. The high-quality image will be saved there. Now you can zoom into your image and retain much more of the original granularity of the data - but we can step up that visualisation further!

The final approach is creating an interactive map through the `leaflet` package and exporting it as an html widget. This produces the highest quality visualisation and also allows the end user to interact with our data (for instance, zoom into specific areas of Bangladesh). First, we need to create a colour palette object called `pal`. Depending on whether we are portraying continuous or categorical data, we will choose the option `colorNumeric` (for continuous) or `colorFactor` (for categorical). We then have to supply our colour palette of interest - in this case `viridis_pal` and define the values that should be assigned to colours, i.e. values of the first raster in the list of processed Malaria Artlas list of rasters `matlas.processed[[1]]`. Finally, we want NA values to appear transparent.

Once we created the colour pallete, we can start creating our interactive map object called `map_access`. This relies on Open Street Map base layers that form a sort of "background" on which we will overlay our accessibility data. I selected `addProviderTiles(providers$CartoDB.Positron)`, which is a nice light greay shaded OSM global map as the background. There are other options (such as colourful backgrounds) that you can explore. On top of that, I am overlaying the raster image with travel time to cities for Bangladesh. I select the colour palette object `pal` and `opacity=0.9`, which controls the transparency of my layer. As the layer already has a specified CRS, we do not reproject. Finally, we add a legend in the viridis colour palette and values of the Malaria Atlas layer. This interactive map is saved in the `html widgets` folder in my working directory. You can choose any location on your local machine to save this file.

### Steps 2-5 - process the demographic data from Meta

To process the demographic data from Meta, we will apply the second function `geo_process2`, which relies on the `sum` algorithm for resampling rather than nearest neighbour interpolation. As before, we will begin by typing the name of the objects that stores a list with all raster files from Meta, `fb.rasters`. This will allow us to view the properties of the following seven datasets for Bangladesh:

1.  Population Density - Children under 5
2.  Population Density - Elderly 60+
3.  Population Density - General
4.  Population Density - Men
5.  Population Density - Women
6.  Population Density - Women of Reproductive Age 15-49
7.  Population Density - Youth 15-24

Just a note that these datasets use population density per pixel as their unit of measurement. This time, we will be applying our second function `geo_process2`, which differs only by the type of algorithm that is used for resampling. Since we are working with *demographic* data of a higher spatial resolution than our base layer, we need to aggregate our data to a coarser grid and the use of the `sum` algorithm will ensure that we are counting all the populations from all higher resolution pixels. An important step here is replacing missing values with zeros, which is done by specifying the argument `value = 0` in the `geo_process2` function. From the documentation, we know that pixels which have been coded as missing in fact have no people in it, i.e. the population density is zero - so we can safely recode. However, the user should always check whether recoding missing values as zeros is appropriate.

As before, we create a list `fblist.processed`, where we will store all the harmonised rasters with demographic information, and apply a loop to process all the seven raster layers with population density for Bangladesh.

```{r meta rasters}
# Check that all the files were picked up by this 
fb.rasters

# Make a list for storing the processed rasters 
fblist.processed <- list()

# Store the length of the list of rasters
num_fb <- 1:length(fb.rasters)  

# Apply the geo-processing function to all rasters. Here we need to recode missing values as 0s
for (i in num_fb) { 
fblist.processed[[i]] <- geo_process2(aux.raster = fb.rasters[[i]], base.raster = base.layer.raster, label = fb.rasters[[i]]@data@names, value = 0)

}

# See the result
fblist.processed

# Compare to base layer
base.layer.raster

# Plot results
plot(fblist.processed[[1]], box=F, axes=F, main = "Bangladesh - Population density (children under 5)")
plot(fblist.processed[[2]], box=F, axes=F, main = "Bangladesh - Population density (elderly 60+)")
plot(fblist.processed[[3]], box=F, axes=F, main = "Bangladesh - Population density (general)")
plot(fblist.processed[[4]], box=F, axes=F, main = "Bangladesh - Population density (men)")
plot(fblist.processed[[5]], box=F, axes=F, main = "Bangladesh - Population density (women)")
plot(fblist.processed[[6]], box=F, axes=F, main = "Bangladesh - Population density (women of reproductive age 15-49)")
plot(fblist.processed[[7]], box=F, axes=F, main = "Bangladesh - Population density (youth 15-24)")

# Create a high quality image 
png(filename="maps/BGD_pop_density_youth.png", res=400, width=2000, height=1500)
plot(fblist.processed[[7]], box=F, axes=F, main = "Bangladesh - Population density (youth 15-24)")
dev.off()


```

The output from simply typing `fb.processed` into your console should show you the attributes of harmonised rasters with demographic data from Meta. The dimensions, resolution, extent, CRS should all be the same as the base layer. In addition, we can see the range of minimum and maximum values for each population density raster.

Visualisations show the population density (per pixel) for 7 population groups for Bangladesh. In all cases, we can see that population density increases around urban areas, such as Dhaka or Chattogram. Unfortunately, the size of this raster image is so large that it does not allow us to visualise it using `leaflet`.

### Steps 2-5 - Process nightlights data

The final dataset that requires harmonisation is the Nightlights Intensity raster from the VIIRS satellite. It contains codified values of radiance per pixel. We will follow the same procedure as with the Malaria Atlas datasets and simply apply our `geo_process` function. In this case, there is no need to recode missing values. The output, `nightlights.raster.processed` should again show dimensions, resolution, extent, CRS that are the same as the base layer. In addition, we can see the range of minimum and maximum values of nightlights intensity.

```{r nightlights raster}
# Apply the geo processing function
nightlights.raster.processed <- geo_process(aux.raster = nightlights.raster, base.raster = base.layer.raster, label = "Nightlights_data", value = 0)

# Check raster
nightlights.raster.processed

# Compare to base layer
base.layer.raster

# Plot raster 
plot(nightlights.raster.processed, axes=F, box=F, main = "Nightlights data in Bangladesh", col=cividis(50))

# Create a high quality image 
png(filename="maps/nightlights_BGD.png", res=400, width=2000, height=1500)
plot(nightlights.raster.processed, box=F, axes=F, main = "Nightlights data in Bangladesh", col=cividis(50))
dev.off()

# Create a colour palette

# View colours from the YlOrRd palette
display.brewer.pal(n = 9, name = 'YlOrRd')

# Hexadecimal color specification 
brewer.pal(n = 9, name = 'YlOrRd')

# Set colour palette 
pal2 <- colorNumeric(c("#1A1A1A", "#FD8D3C", "#FED976", "#FFEDA0", "#FFFFCC"), values(nightlights.raster.processed),
  na.color = "transparent")


# Create an interactive leaflet map
map_night <- leaflet() %>%
 addProviderTiles(providers$CartoDB.Positron) %>%
  addRasterImage(nightlights.raster.processed,  colors = pal2, opacity=0.9)%>%
  addLegend(pal = pal2, values = values(nightlights.raster.processed), title = "Nightlights intensity")

# Let's have a look
map_night

# Save map as widget
saveWidget(map_night, file="html widgets/BGD_nightlights.html")

```

A simple base R plot shows a visualisation of nightlights intensity (measured as radiance per pixel) for Bangladesh using the `cividis` colour palette. Urban areas such as Dhaka appear to have a higher radiance of nightlights, though it is a little difficult to see given the low default resolution of plots in R. Exporting the plot as a higher-quality image allows for a better visual result.

This could be a good opportunity to introduce custom-made colour palettes into our plots. Instead of using the `cividis` palette, let's say I have a very specific idea of how I want to portray my data: black for complete absence of night lights (intuitively, areas that would appear dark in a satellite image), and gradually building shades of yellow that could indicate the presence of lights at night. [R Color's Brewer](https://r-graph-gallery.com/38-rcolorbrewers-palettes) offers quite versatile palettes for representing all sorts of data: sequential, diverging, and qualitative - and we can construct our own palettes based on these colour schemes. Using the `display.brewer.pal` command, I can view the colours associated with each palette and using `brewer.pal` I can obtain their codings. In this case, I want to see the `YlOrRd` palette and extract the yellow shades, while setting black as my first colour. I save this scheme in the `pal2` object and supply it to `leaflet`. The end result, an interactive html map with my custom colour palette, displays nightlights more clearly.

This concludes our harmonisation of geo-spatial covariates - we now have three lists of rasters that are ready to produce zonal statistics: `matlas.processed`, `fblist.processed`, `nightlights.raster.processed`. As the WorldPop datasets on topography have already been harmonised, there is no need to process them. But we can still play around with some visualisations.

### Steps 2-5 - Inspect pre-harmonised World Pop datasets

The WorldPop datasets already have the same spatial resolution, extent, and dimensions as our base layer. They are ready to produce zonal statistics. Typing the list of raster objects, `worldpop.rasters`, will show that we have 16 raster objects, with the same attributes as the base layer, all showing distances to different types of Land Cover classes in Bangladesh. We don't need to further process this geodata in any way - but let's inspect it by creating simple plots.

As an example, let's produce a simple base R plot of the 11th raster object from that list, which shows distance to the closest road. Even without any processing, the plot comes out nicely formatted, as World Pop has pre-harmonised this data. Coastal areas are clearly the furthest away from any road. Unfortunately, the raster object is too large to create an interactive map using `leaflet`.

```{r worldpop rasters}
# See that all the files have loaded
worldpop.rasters

# Check they match the base layer
base.layer.raster

#Plot examples of WorldPop datasets
plot(worldpop.rasters[[11]], box = F, axes = F, main = "Distance to the closest road Bangladesh", col=viridis(50))

```

### Step 6 - Calculation of zonal statistics

This brings us to our final step - calculating zonal statistics, i.e. the values of min, max, mean, sd etc. of all the geo-spatial datasets at upazila level, which will serve as inputs into poverty estimation in Section B. For this, we will use our harmonised rasters from steps 2-5: `matlas.processed`, `fblist.processed`, `nightlights.raster.processed`, and `worldpop.rasters`, as well as the shapefile containing the administrative level 3 (upazila) boundaries in Bangladesh: `shapefile.zone.plot.`

We begin by putting all of our rasters into one list, `full.raster.list` and creating a stack of rasters called `raster.stack`, which is essentially a collection of rasters with the same extent and spatial resolution that can be stored in one file. You can think of those layers as being "stacked" on top of each other. This format will make it easier to calculate zonal statistics as we only have to supply one object to the function (as opposed to doing it separately for 24 raster layers).

Now we can calculate our zonal statistics: mean, minimum value, maximum value, sum, standard deviation, and the number of observations. We will make use of the `exact_extract` function, which is the quickest and most computationally efficient way of obtaining key statistics from geo-spatial data at the chosen administrative level. Supplying our `raster.stack` with all geo-spatial data and `shapefile.zonal.plot`, which contains upazila boundaries in the zonal function will calculate all the statistics of interest. We will save those in a data frame format (which should be fairly familiar!), with column names indicating the zonal statistic for each geo-spatial layer. We will also include a column with the administrative code for each upazila, given by `ADM3_PCODE`. The name of this final data frame is `BGD.zonal.stats`.

```{r zonal stats}
# Make a list with all the rasters 
full.raster.list <- c(matlas.processed, fblist.processed, worldpop.rasters, nightlights.raster.processed)

# Check rasters 
full.raster.list

# Create a raster stack
raster.stack <- stack(full.raster.list)

# Apply exact extract function
BGD.zonal.stats <- exact_extract(raster.stack, shapefile.zone.plot, c('min', 'max', 'mean', 'sum', 'stdev', 'count'), stack_apply=TRUE, append_cols="ADM3_PCODE")

# Save zonal stats as an RData file
#save(BGD.zonal.stats, file = "BGD.zonal.stats.RData")

# Save my workspace
#save.image("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/EMDI SAE/Geo-spatial Bangladesh data/Quarto github workspace update.RData")

```

Congrats! You have made it to the end of Section A on processing geo-spatial covariates. Now you can use them to predict poverty rates for every upazila in Bangladesh, which will be done in Section B.

## 4. Caveats, problems, things to look out for

**Replication:** This guide is intended to be replicable with various types of geo-spatial data sources and country contexts - we would welcome all such replication exercises and hope that our material can be used to produce small area estimates of poverty in countries other than Bangladesh! That said, the exact raster manipulation as well as processing steps needed will depend on your specific geo-spatial data. Always check the key elements of your raster data to inform the processing you will need to do - our procedure is not to be simply copy pasted (though it provides many useful functions you can work with). CRS, spatial resolution, extent, number of cells, and dimensions are always good things to check.

-   

-   **Time period of geo-spatial data**: whenever possible, try to obtain geo-spatial data for the same time period as the household survey data that contains the poverty variable (or any other outcome you are interested in modelling). If that is not possible - say that you can only access geo-spatial data that is older than your poverty data - you should consider whether this information is outdated, or could still be considered relevant (for instance the elevation of mountain ranges is unlikely to have changed much over time, but demographic data will change quite quickly).

-   **Choice of poverty predictors**: This list of predictors of poverty is only indicative. You should consider, when replicating the poverty modelling exercise in your country context, what sources of geo-spatial data are available and relevant to poverty prediction.

-   **Level of administrative disaggregation**: Although in this example we consider administrative level 3 in Bangladesh, small area estimation of poverty using geo-spatial data can be replicated at any other administrative level (higher or lower) - and this will vary depending on the country. Note however that the level of aggregation will inform your modelling choices (see Section B for more details).

-   **Resampling algorithm**: The choice of the algorithm used for resampling matters - and depends on the information represented by your geo-spatial data, as well as whether you are aggregating (moving to a lower spatial resolution), or disaggregating (moving to a higher spatial resolution). For example, when you aggregate demographic data - you should use the "sum" algorithm. More details on this can be found in Section 3.

# Section B

# Introduction

Section A of the guide showed you how to gather and harmonise geospatial data from online in order to make it into a single dataframe ready for analysis. However, as a reminder, the complete estimation procedure of the Fay-Herriot model is an 8 step process! Section B will explain to you how you go from the geospatial dataset you have produced in the first section of the guide, to producing poverty estimates for every upazila in Bangladesh and displaying this information on a map like the one seen below!

!['Poorest' indicates the probability that a household in that upazila is ranked in the bottom quintile of wealth in Bangladesh.](images/FH%20Bangladesh.png){fig-align="center" width="437"}

The complete steps of the estimation procedure are as follows:

Step 1: Gathering the geospatial data and compiling it into a harmonised dataset.

You can either start with the geospatial co-ordinates you generated in section A, or, if you just want to practice the estimation procedure, you are able to download our pre-processed dataframe [here.]{.underline} Section A of our guide explains how to compile this geospatial dataframe in detail.

Step 2: Carrying out a spatial join of the datasets and then generating direct estimates of poverty and their sampling variance for the in-sample observations.

Our first step here is carrying out a spatial join. Each cluster of households in the DHS survey is uniquely identified but we do not know which upazila they reside within. In order to solve this issue, we overlay the distribution of clusters over a shapefile of the whole country and assign upazila codes to the cluster depending on which polygon they reside within.

Then, in order to account for the fact that surveys often employ complex sampling designs, with elements of stratification and multistage sampling with unequal probabilities, we need to calculate direct estimates of our variable of interest (in our case this is poverty) and estimates of their variances. This is in order to prevent bias being introduced into the estimation. We carry out this step using household wealth data downloaded from the DHS survey.

Once we generate the direct estimates and their variances. We then smooth the variances using a generalised variance function. This improves performance of an area-level model by reducing noise, stablising estimates as well as accounting for uncertainty in the data because of the limited sample size which is an inherent problem when doing prediction using area-level models.

Step 3: Merging the geospatial data to the direct estimates of poverty generated in step 2.

Now we need to combine the geospatial data from section A of the guide and the direct estimates and their variances that you generated in step 2 of the process. In order to combine this information into one big dataset we need a unique common identifier and for this we merge the observations using the unique domain identifier 'upazila.code'. This code uniquely identifies every single administrative level 3 (upazila level) area in Bangladesh so that R knows which area you are referring to when looking at the values of household poverty and the geospatial covariates. This also is the area code we use later to merge the dataset with the shapefile in order to display our poverty estimates on a map!

Step 4: Selecting the geospatial covariates with the most predictive power

The initial large dataframe we generated in section A with over 150 geospatial variables is unwieldy and following the 'principle of parsimony' we want fewer variables. In order to prevent issues such as overfitting we initially trim the model to include fewer covariates. For this we use a selection criterion (AIC/BIC/KIC) to find only the geospatial covariates with the highest predictive power on predicting the variable of interest.

We perform a BIC stepwise selection criterion using all of the available geospatial covariates to see which 15-20 variables perform the best in predicting poverty rates in an area.

Step 5: Generating the model and checking model assumptions.

We now use the geospatial covariates selected in step 4 by the selection criterion in a basic model on a raw scale in order to test the key assumptions of the model and assess the predictive power. For example, the model assumes normality of the error terms which, if violated, introduces bias into the estimation so it is necessary to show that all the necessary assumptions hold.

If the assumptions do not hold with the model in its current format then it is important to carry out a transformation on the target variable. For example log transformations are traditionally popular with income data in order to mitigate issues such as skewed data. In our case, since we are modelling a proportion or a percentage, we use an arcsin transformation. This kind of transformation is common for dealing with data that is bounded between 0 and 1 and is very effective at stabilising the variance of data that follows a binomial distribution.

We then use a variety of visual aids such as plots as well as regression tests to ensure that we are satisfied that all our assumptions hold after a transformation.

Step 6: Carry out the FH model prediction for our out of sample estimates.

In this step we carry out the FH modelling procedure using the model we have developed above that satisfies our assumptions, in order to generate our out of sample wealth estimates. The FH model adopts a bootstrap approach in order to predict out of sample estimates. This is an iterative resampling method which provides estimates of variability and uncertainty without requiring strong assumptions about the data distribution.

The FH model has been trained on in-sample domains which have both geospatial and household data available. However, by definition, only geospatial data is available in out-of-sample domains. By learning the relationship between the most significant geospatial covariates and the variable of interest (as determined by the selection criterion in step 4), the model can estimate what the probability is that a household is in poverty would be in an area that has not been surveyed in the DHS.

Step 7: Comparison of FH model estimates and variances with the ground-truth direct estimates and variances from step 2.

In this step we compare the direct point poverty estimates we generated in step 2 using the DHS data with the FH model estimates that we generated in step 6. This is done both graphically and with a Brown test. This gives some indication of the precision of the model. If we are satisfied with the similarity of the estimates, we can move on to the final stage.

Step 8: Visualisation of the results.

The final step of the process is visualising the results. To do this we use the main Bangladesh shapefile again which includes the co-ordinates of the country's border lines and includes the polygon shapes for each upazila. We combine this shapefile with our dataset using the unique upazila district code and then visualise the relative wealth in each domain by using a gradient colour scheme.

# Estimation

### Using the Fay Herriot model to predict poverty in Bangladesh

### Step 1: Gathering your geospatial data

This step of the estimation procedure is the material covered in part 1 of the guide. Ensure to save this dataset to your working directory since it is used throughout the process.

Alternatively, for practice following the remainder of the guide, or to ensure you have followed the steps correctly and your data looks the same, you can download our pre-processed estimates [*here*]{.underline}.

### Step 2: Spatial joining of the datasets and then generating direct estimates of poverty and their sampling variances

This step is necessary in order to generate our poverty estimates for the in sample areas. Since the DHS survey employs a sampling strategy we cannot take the values they produce at face-value and first have to adjust them to make them to make them more representative of the whole sample.

The first step is to download the Bangladesh household poverty data from the [DHS website](https://dhsprogram.com/Countries/Country-Main.cfm?ctry_id=1&c=Bangladesh&Country=Bangladesh&cn=&r=4). From this link you will need to click on the 2014/17 Bangladesh survey. The datasets we are interested are the household recode since this gives the household data on poverty in the country. We are also interested in the Bangladesh shapefile dataset since this gives the co-ordinates of every upazila. We then harmonise these two datasets in order to portray the data on a map. To download these files you will need to register an account with DHS and give a brief explanation of what you intend to do with the data. For this you can explain that you wish to generate spatially disaggregated poverty estimates for the country.

::: callout-tip
Top tip: when requesting access to the data, remember to tick the box requesting geospatial data as well!
:::

::: callout-note
When applying to the DHS for data they say that they will get back to you with their decision within 2 business days (for us it was just a matter of a couple of hours) but be mindful that you should apply for the data a couple of days before you decide to follow this guide.
:::

![The ticked boxes indicate the 2 required files from DHS](images/DHS%20file%20download.png){fig-align="center" width="561"}

We now need to harmonise these two files (the geographic data and the household recode) by doing a spatial join. This generates the unique domain identifier for each observation so that you are able to identify each observation by both its upazila as well as its cluster.

So now you have downloaded the two datasets specified in the screenshot above, we need to merge them together. We start by loading the 2014 DHS Shape file which gives us the GPS coordinates of the centroid of the DHS clusters. We keep only relevant variables from the shapefile so the dataset does not get too large and unwieldy.

```{r dhs 2014 geo}
# Set working directory
setwd("C:/Users/nickl/OneDrive - Oxford Policy Management Limited/EMDI SAE/Bangladesh household survey/DHS 2014/BD_2014_DHS_06132023_156_196138/GPS coordinates/DHS HH GPS/BDGE71FL")

# Open the shape file from 2014 DHS data
GPS.DHS.2014 <- st_read("BDGE71FL.shp")

# Get column names
colnames(GPS.DHS.2014)

# Keep only relevant variables - longitude, latitude, geometry, DHS cluster ID
GPS.DHS.2014 <- GPS.DHS.2014[c("DHSCLUST","LATNUM","LONGNUM", "geometry")]

```

Next we load the DHS Housheold Recode 2014. This dataset contains information on the household wealth index which is what we use to make our variable of interest and this can be merged with the GPS shapefile using the DHS cluster number. We keep the variables: household ID, wealth index, DHS cluster ID, household number, sampling weight and wealth quintile rank as well since these are necessary for us to include in our modelling. We also rename them in order to make the process clearer when using the variables later.

```{r}
# Set working directory
setwd("C:/Users/nickl/OneDrive - Oxford Policy Management Limited/EMDI SAE/Bangladesh household survey/DHS 2014/BD_2014_DHS_06132023_156_196138/BDHR72DT")

# Load the 2014 DHS Household recode module
DHS.2014.HH <- read_dta("BDHR72FL.dta")

# Renaming the variables of interest - household ID, wealth index, DHS cluster ID, household number, sampling weight, wealth quintile rank
DHS.2014.HH <- DHS.2014.HH %>% rename("DHSCLUST" = "hv001",
                                      "wealth_quintile_rank"= "hv270",
                                      "hh_sample_weight" = "hv005",
                                      "hh_number" ="hv002",
                                      "wealth_index" = "hv271",
                                      "year" = "hv007",
                                      "strata" = "hv023")
# Keep only the variables of interest as specified above
DHS.2014.HH <- DHS.2014.HH[c("year", "hhid", "DHSCLUST","wealth_index","hh_sample_weight", "hh_number", "wealth_quintile_rank", "strata")]

# Join the DHS household recode with GPS shapefile
DHS.2014.geocoded <- left_join(DHS.2014.HH, GPS.DHS.2014)

```

We are now able to perform a spatial join to match the household clusters to upazilas. We first use the 'longnum' and 'latnum' to make coordinates for each indivudal upazila. We then ensure that the 2 shapefiles have the same projection so that they can be joined without conflict. We then perform a spatial join between the spatial points data frame from DHS and a spatial polygons data frame from the Bangladesh shapefile. This shapefile is the same one you downloaded in section A, step 3 saved as: "bgd_admbnda_adm3_bbs_20201113.shp". This function will check whether each spatial point (geographical coordinate) from the DHS shapefile falls inside any polygon from the Bangladesh shapefile and return a data frame with successfully matched information from both spatial objects.

::: callout-note
If you are only following section B of the guide and using our pre-loaded harmonised geospatial covariates dataset then you will need to download the shape files here: [Bangladesh - Subnational Administrative Boundaries - Humanitarian Data Exchange](https://data.humdata.org/dataset/cod-ab-bgd). You will need to download the zip file entitled "bgd_adm_bbs_20201113_SHP.zip".
:::

```{r spatial join}
# Turn data frame into spatial points data frame
coordinates(DHS.2014.geocoded) <- ~LONGNUM+LATNUM

# Set projection as the same for both data frames - based on the projection from the Bangladesh shapefile
proj4string(bgd_admin3_shp) <- CRS("+proj=longlat +datum=WGS84 +no_defs")
proj4string(DHS.2014.geocoded) <- CRS("+proj=longlat +datum=WGS84 +no_defs")

# Perform the spatial join between the DHS shapefile and the Bangladesh shapefile. 
ID <- over(DHS.2014.geocoded, bgd_admin3_shp)
DHS.2014.geocoded@data <- cbind(DHS.2014.geocoded@data , ID )

# Turn the spatial points data frame into a regular data frame 
bgd_dhs_geo_2014 <- as.data.frame(DHS.2014.geocoded)

# Remove the geometry column and null columns for csv formatting 
bgd_dhs_geo_2014 <- bgd_dhs_geo_2014[ , -which(names(bgd_dhs_geo_2014) %in% c("geometry" , "ADM3_REF", "ADM3ALT1EN", "ADM3ALT2EN"))]

# Export as csv file
write.csv(bgd_dhs_geo_2014, "bgd_geo_2014.csv")
```

![Source: Aaron Maxwell University of West Virginia](images/West%20Virginia%20spatial%20join.png){fig-align="center" width="534"}

This figure, showing the location of airfields in West Virginia, is an example of how a spatial join works. The main shapefile has the county lines loaded and the outline of the shape representing the state border. The second shapefile shows every airfield and their respective gps coordinates. When the two shapefiles are projected in the same perspective a spatial join is then able to tell you in which county the airfield is based. This is the same concept as our study with the spatial join informing us in which upazila each cluster of households is located based on the GPS coordinates.

Once we format the new dataframe correctly and remove the unnecessary columns, the new dataframe is represented by bgd_geo_2014.csv. This dataframe combines the required variables from the household recode module for the 2014 DHS, the geo-location of clusters of households, and upazila codes to which the clusters belong. This dataset we are now using has information on: wealth index raw score, quintile ranking of the wealth index, strata, household weights, upazila, number of clusters and GPS location of clusters.

::: callout-note
The GPS coordinates we use correspond to the centroid of a cluster of households, which has a random offset of 2km in urban areas and 5-10km in rural areas to preserve the privacy of households. However, the DHS note that efforts have been made for offsets to be specified in a way that preserves the original administrative units within which HHs reside - therefore this should not affect our matching of households to upazilas.
:::

Our next step is to generate our variable of interest. In our case we are looking to create a poverty map so we will be creating a variable using the wealth index, however, this could easily be adapted to create indicators for other key macroeconomic objectives. We create the variable 'binary_wealth'. This is a binary variable that is set as equal to 1 if the household was defined as being in the lowest quintile of wealth in Bangladesh by the DHS survey. Otherwise the variable is set as equal to 0. By definition 20% of households across the country will be in this bottom quantile but you will be able to see which areas are poorer by seeing which areas across the country have a higher probability of a household being in this bottom quintile of wealth.

We also create a variable to determine how many clusters were sampled in each upazila because depending on this, the estimates will be more or less precise due to the change in sample size. The number of clusters in each upazila is also necessary for our variance estimation in the next stage of the estimation procedure.

::: callout-tip
The dataset we use this for this is the "bgd_geo_2014.csv" dataset we just created in the spatial join so ensure you have this file saved to your working directory!
:::

First you need to install and load necessary packages for the estimation:

```{r}
library(survey)
library(nlme)
library(emdi)
library(dplyr)
library(sae)
```

We then run the code as follows:

```{r load DHS data, include=FALSE}
# Load 2014 geo-referenced DHS data
Ban_data <- read_csv("C:/Users/nickl/OneDrive - Oxford Policy Management Limited/EMDI SAE/Bangladesh household survey/DHS 2018/bgd_geo_2014.csv")

# Check how many observations there are there in each cluster
Ban_data <- bgd_geo_2014 %>%
  group_by(ADM3_PCODE) %>%
  mutate(num_clusters = n_distinct(DHSCLUST))

# Display values in a table
frequency_table <- table(Ban_data$num_clusters)

print(frequency_table)

# Calculating the numbers of clusters per Upazila
upazila_cluster_count <- Ban_data %>%
  group_by(ADM3_PCODE) %>%
  summarize(num_clusters = n_distinct(DHSCLUST))


# Summarize the count of clusters per Upazila
cluster_summary <- table(upazila_cluster_count$num_clusters)

# View the summary table
print(cluster_summary)

# Constructing a binary variable for whether a HH belongs to the lowest wealth quintile.
Ban_data$binary_wealth <- 0

# Replace with 1 if the HH belongs to the lowest quintile
Ban_data$binary_wealth[Ban_data$wealth_quintile_rank == 1] <- 1

```

Now we know how many clusters are in each upazila, we are able to generate the direct estimates and the sampling variances. The method varies if there is only 1 cluster per upazila compared to if there are multiple clusters, since by definition if there is only one cluster in the upazila there will be the same variation in the cluster as in the whole upazila. Therefore,if there is only one upazila in the cluster, we use a design effect. If there is more than 1 cluster in the upazila we are able to calculate using the survey() package.

First of all we calculate the direct estimates and the sampling variances for every upazila using the survey package, regardless of how many clusters are in the area. We set the vartype = "se" rather than confidence intervals because we need the variance later in order to generate the poverty estimates. The second step is that we will then overwrite the direct estimates and variances for the upazilas which have only one cluster using the second method of the design effect.

**Direct and standard error estimation for all upazilas (irrespective of the number of clusters)**

```{r direct and se estimation all upazilas}
# Create a survey design object, accounting for household weights, strata, and clusters 
DHS.design <- svydesign(ids = ~ DHSCLUST,
                        weights = ~ hh_sample_weight,
                        strata = ~ strata,
                        data = Ban_data)

de <- svyby(~ binary_wealth,
            by = ~ADM3_PCODE,
            design = DHS.design,
            svymean,
            vartype=c("se"))

# Check standard errors
summary(de$se)


```

We now look at the direct estimates and sampling variances for observations where there is only one cluster in an upazila. We use a design effect to do this. The design effect of a single-stage cluster sample, assuming equal selection probabilities and clusters of size M, is given by design effect (henceforth deff or de) = 1 + (M - 1)\* ICC where ICC is the intra-cluster correlation coefficient for the variable of interest. ICC is estimated by the variance of the binary_wealth variable in the cluster divided by the variance of the poorest variable of the cluster added to the variance of the household. The DHS survey design involves a second stage of selection of a group of 30 households out of those in the cluster (113 on average) therefore we have set M = 4.

The design effect is effectively a multiplier that quantifies the impact of a survey design on the variability of estimates. It indicates how much larger the sample size would have to be compared to a simple random sample to achieve the same level of precision as the DHS survey with its complex design. As a result of this, in the modelling procedure later, we divide the number of households per upazila by the design effect in order to calculate the effective sample size per upazila.

We fit the model using the lme function from the package nlme. STRS stands for stratified simple random sampling. We use an lme model (linear mixed effects) since it incorporates both fixed and random effects. This takes into account the overall relationships between variables which are the same for all upazilas in the data (fixed effects), and also takes into account variations in data across different upazilas that are specific to a certain upazila (random effects).

**Estimation of design effect for upazilas with one cluster**

```{r design effect estimation}
# Fitting a linear model with just an intercept
fit.rs <- lme(binary_wealth ~ 1,
              random = ~1 |
                DHSCLUST, method = "ML",
                data = Ban_data[Ban_data$num_clusters ==
                                1, ])
# Variance covariance matrix
vcomp <- as.numeric(VarCorr(fit.rs))

# Save this as a matrix
test.matrix <- VarCorr(fit.rs)
test.matrix

# Calculate the design effect 
deff.1clus <- 1 + (4 -1) * vcomp[1]/(vcomp[1] + vcomp[2])

# Check design effect 
deff.1clus

# deff is 1.739737 in our case and 1.72 in Nikos's case. We also get an Intercluster Correlation (ICC) of 0.24, same as Nikos.

# Calculate standard errors

# Create a survey design object for upazilas with just one cluster 
design.STSRS <- svydesign(id = ~1,
                          weights = ~hh_sample_weight,
                          strata = ~strata,
                          data = Ban_data)

de.STSRS <- svyby(~binary_wealth,
                  by = ~ADM3_PCODE,
                  design = design.STSRS,
                  svymean,
                  vartype=c("se"))

de.STSRS <- dplyr::select(de.STSRS,
                          ADM3_PCODE, se.STSRS = se)

# Create an object with upazila codes and number of clusters
sizes <- Ban_data[, c("num_clusters", "ADM3_PCODE")]

de <- merge(de, sizes,
by = "ADM3_PCODE") %>%
merge(de.STSRS, by = "ADM3_PCODE") %>%
mutate(se.imp = ifelse(num_clusters ==
1, se.STSRS *
sqrt(deff.1clus),
se))


# Remove duplicates
final_de <- de[!duplicated(de$ADM3_PCODE), ]
View(final_de)


```

Pre-smoothing this is what the CV (coefficient of variation) looks like:

```{r}
#  2. Test our data  #
# Turn number of clusters into factor
final_de$num_clusters_factor <- as.factor(final_de$num_clusters)

# Calculate the coefficient of variation in our data
final_de$cve <- (final_de$se.imp / final_de$binary_wealth) 

# Plot 
ggplot(final_de, aes(x=binary_wealth, y=cve, color=num_clusters_factor)) + geom_point() 
```

::: callout-note
The coefficient of variation is simply calculated as the standard deviation over the mean. A CV of below 20% or 0.2 is often required for publication.
:::

**Smoothing the variance estimates using a Generalised Variance Function**

To reduce the volatility in the coefficient of variation as seen in the graph above, we use an additional smoothing step that involves the use of Generalised Variance Functions. Smoothing the variance estimates may be needed in order to improve the performance of area-level models. Smoothing involves the initial variance estimates being replaced by corresponding predicted values produced under a linear regression model.

The predictors we use in this model include the direct estimates of our variable of interest 'binary_wealth', the square of the direct estimates, the number of clusters representing each upazila, the square of the number of clusters and the average sampling weight for a household in each upazila. In order to avoid undue impact of influential observations, the model is fitted by using only the data of upazilas with fewer than 5 clusters. This is because, as seen in the plot and frequency tables above, there are relatively few upazilas with more than 5 clusters in them so a model estimating standard errors might be overly influenced by large outliers.

The steps of this process are as follows:

1.  Calculate the average HH weight for each upazila

2.  Fit a linear model where the outcome is an estimate of the standard errors and the predictors are: direct estimates of binary_wealth, the square of the direct estimates, the number of clusters representing each upazila, the square of the number of clusters and the average sampling weight for a household in each upazila. This process is done using a subset of the upazilas that have fewer than 5 clusters.

3.  Make a data frame with fitted values from this linear model, upazilas to which those correspond, and the number of clusters.

4.  Replace the original CV (coefficient of variation) by fitted values for upazilas that have 2-4 clusters.

```{r smoothing}
# Calculate the average sampling weight for households in an upazila
Ban_data <- Ban_data %>%
  group_by(ADM3_PCODE) %>%
  mutate(avg_hh_weight = mean(hh_sample_weight))

# Create a data frame with just the average hh weight and upazila code
avg_weight_df <- Ban_data[, c("avg_hh_weight", "ADM3_PCODE")]

# Remove duplicates
avg_weight_df <- avg_weight_df[!duplicated(avg_weight_df$ADM3_PCODE), ]

summary(final_de)

# Merge with the final_de data
final_de <- left_join(final_de, avg_weight_df)

# Make a subset for upazilas with fewer than 5 clusters
final_de_subset <- subset(final_de, num_clusters < 5)

# Remove observations where standard errors are 0 (this does not tell us anything about the variability of the estimate since there is no variation)
final_de_subset <- subset(final_de_subset, se.imp > 0)

# Fitting a linear model
model1 <- lm(se.imp  ~ binary_wealth + I(binary_wealth^2) + num_clusters + I(num_clusters^2) + avg_hh_weight,
                data = final_de_subset, na.action=na.omit)

summary(model1)

# In our case the model explains 59% variation in data and in Nikos's case 62%


# Save fitted values in a data frame - initial variance estimates should be replaced by predicted values from this model
fitted <- as.data.frame(model1[["fitted.values"]])

summary(fitted)

# Save upazila codes in a list
upazilas <- final_de_subset$ADM3_PCODE

# Add to the fitted data frame 
fitted$ADM3_PCODE <- as.character(upazilas)

# Get column names
colnames(fitted)

# Rename column
names(fitted)[names(fitted) == 'model1[["fitted.values"]]'] <- 'se.GVF'

# Have a look 
summary(fitted$se.GVF)

# Save number of clusters in a list
clusters <- final_de_subset$num_clusters

# Add to the fitted data frame
fitted$num_clusters <- as.numeric(clusters)

# Now look at the summary
summary(fitted$se.GVF)


# Left join with final_de dataframe
final_de <- left_join(final_de, fitted)

# Replace NA values of se.GVF with the previous standard errors - this is equivalent to smoothing for all upazilas that have 1-4 clusters
final_de$se.GVF <- ifelse(is.na(final_de$se.GVF), final_de$se.imp, final_de$se.GVF)

# Now re-calculate CV
final_de$cve.2 <- (final_de$se.GVF / final_de$binary_wealth) 

# Plot 
ggplot(final_de, aes(x=binary_wealth, y=cve.2, color=num_clusters_factor)) + geom_point() 

```

You can see in the graph above that the CV is much smoother and this is much more appropriate when predicting variables using area level models since it reduces noise, stabilizes estimates and enhances model performance. The above model explains 62% of the variability present in the data. After fitting the model, only the variances for upazilas represented with fewer than 5 clusters are smoothed.

Now our datasets that we will use for the FH modelling are ready! We have the 'final_de' dataframe which includes our variable of interest and the standard errors we have estimated using the generalised variance function (se.GVF). The other dataframe is the 'BGD_zonal_new_version' which includes our geospatial variables.

### Step 3: How to combine your datasets. Getting your geospatial data and direct estimates of poverty in the same dataframe.

We use the merge command to merge two dataframes together. We then use the column bind command to create the variable new_df\$se.GVF\^2 which is standard errors squared from the generalised variance function (GVF). This is used as the variable "vardir" which is an important argument in the FH model.

```{r}

BGD_zonal_new_version <- read_csv("C:/Users/nickl/iCloudDrive/Desktop/Oxford Policy Management/Bang_V2/BGD.zonal.new.version.csv")

View(BGD_zonal_new_version)

# Creating new variable 'upazila.code' from ADM3_PCODE to make merge compatible  
BGD_zonal_new_version$upazila.code <- substring(BGD_zonal_new_version$ADM3_PCODE, 3)

# Merging the geospatial data and the household poverty data with updated variance estimation from step 2
new_df <- merge(BGD_zonal_new_version, final_de[, c("upazila.code", "binary_wealth", "se.GVF", "num_clusters")], by = "upazila.code", all.x = TRUE)

# Creating new variable with the
new_df = cbind(new_df, new_df$se.GVF^2)

attach(new_df)

View(new_df)
```

### Step 4: Covariate selection for initial model.

This step follows the principle of parsimony in order to slim down the model to find only the geospatial covariates that have the highest predictive power.

To do this we first create a new subsetted dataframe, excluding the variables that were used to create the wealth index or the standard errors of the wealth index. Since these 1) are not geospatial covariates and 2) will clearly be highly correlated with our 'binary_wealth' indicator of interest. We also exclude the upazila.code column since we only want to include the geospatial covariates to see which are the most significant predictors of poverty.

We then fit a full linear regression model using the new dataframe.

```{r}
# Subset of the dataframe
new_df2 <- new_df[,c(3:165)]

# Fitting a linear regression model
full_model2 <- lm(binary_wealth ~ ., data = new_df2)
```

We then perform stepwise selection based on BIC criterion to narrow down the number of variables from 150+ to around 15-20. K refers to the number of degrees of freedom used for the penalty, or in other words, how selective the criterion will be when including a variable in the model.

```{r}
stepwise_model2 <- stepAIC(full_model2, direction = "both", trace = FALSE, k = log(nrow(new_df2)))
```

![Source: https://thaddeus-segura.com](images/backward-stepwise-algorithm.png){fig-align="center" width="367" height="365"}

This is an illustrative example showing how a basic backwards model selection would work. Forward stepwise selection is the same in reverse, it starts with the null model and adds the most significant variables until the stopping criterion is reached. Since we are running a stepwise model it uses both forward and backward steps to iteratively add and remove variables until no further changes need to be made to the model.

We can then view the selected geospatial variables which we will use in our cursory model.

```{r}
summary(stepwise_model2)
```

### Step 5: Fitting the model

Our first step of fitting the model is running a cursory model. For this we use the variables that were selected to be most significant by the stepwise selection procedure above. We also carry out the estimation with no transformation on a raw scale to see if the model offers reasonable predictive power.

We can then run a couple of model diagnostic regressions to see how the raw scale fits the data and to see if the vital assumptions are satisfied.

Raw scale estimation:

```{r}
fh_raw_scale <- fh(fixed = binary_wealth ~ min.osm_dst_waterway_100m_2016_BGD + 
    max.bgd_women_2020 + max.bgd_women_of_reproductive_age_15_49_2020 + 
    max.esaccilc_dst_water_100m_2000_2012_BGD + mean.X201501_Global_Travel_Time_to_Cities_BGD + 
    mean.esaccilc_dst_water_100m_2000_2012_BGD + mean.osm_dst_road_100m_2016_BGD + 
    sum.X201501_Global_Travel_Time_to_Cities_BGD + sum.bgd_children_under_five_2020 + 
    sum.bgd_general_2020 + sum.bgd_men_2020 + sum.bgd_women_of_reproductive_age_15_49_2020 + 
    sum.bgd_youth_15_24_2020 + sum.osm_dst_road_100m_2016_BGD + 
    sum.Nightlights_data + stdev.esaccilc_dst130_100m_2014_BGD + 
    count.X201501_Global_Travel_Time_to_Cities_BGD + count.X202001_Global_Motorized_Travel_Time_to_Healthcare_BGD + 
    min.esaccilc_dst150_100m_2014_BGD, vardir = "new_df$se.GVF^2",
          combined_data = cleaned_df, domains = "upazila.code", method = "ml",
          MSE = TRUE, mse_type = "analytical")

summary(fh_raw_scale)
```

When looking at the model summary the adjusted R-squared is 0.57. This might seem like reasonable predictive power.

However, we also need to look at normal probability plots of residuals and random errors to see if there is a clear departure from normality.

To do this, we generate 2 output plots showing:

1.  Normal Q-Q plot showing level 1 residuals

2.  Normal Q-Q plot showing the random effects.

There are two plots since we are using an lme so the first plot shows the residuals of the fixed effects and the second plot shows the distribution of random effects. If the plot follows a straight line then it suggests that the residuals/ random effects are normally distributed which is a key assumption in our model.

```{r}
lev1 = fh_raw_scale$model$real_residuals
lev2 = fh_raw_scale$model$random_effects
qqnorm(lev1)
qqnorm(lev2)                   
```

Especially in output plot 1 this graph resembles more of an upwards curve rather than a 45 degree line which we would hope for. This is our first warning sign that potentially the current model is not a great fit for the data.

Therefore, since we are not convinced, we also need to critically evaluate the next diagnostic check. This next function will generate direct and model-based point estimates of our variable of interest 'binary_wealth' as well as corresponding variances and MSEs (mean squared errors). Note that this is only possible if the MSE argument was set equal to TRUE when fitting the area-level model. This second diagnostic model is important to evaluate if we are going to use the model for predictive purposes.

```{r}
est_raw = estimators(fh_raw_scale,
                     MSE = TRUE)             
summary(fh_raw_scale$ind$FH)
```

We notice that the model-based point estimates include negative predicted values (min = -2.19) for our variable of interest despite the fact that we are modelling a proportion that obviously should be between 0 and 1. This is a consequence of using a model that does not fit well and illustrates the importance of using model diagnostics.

As a result, in order for the model to better fit the data we are going to try using an arcsin transformation. The arcsin transformation is often used when data is in the form of percentages or proportions that are close to either 0 or 1. The transformation stabilizes the variance of data that follows a binomial distribution, especially when the proportions are close to the boundaries of 0 or 1, as is the case here since 'binary_wealth' is a variable indicating the percentage or proportion of households in each area that are in poverty. Using this transformation requires additional arguments, namely the effective sample size which, as discussed earlier, is defined by the sample size divided by the design effect, as well as the type of backtransformation.

First of all, in order to do this, we need to generate the argument relating to the effective sample size. To do this you first calculate how many households have been surveyed in each upazila from the initial DHS survey. You then load the design effect that we calculated in step 2. The number of households per upazila is then divided by the design effect in order to calculate the effective sample size.

You can then merge this with the new_df we are using to make sure we have all these variables in one dataset.

```{r}
# Calculating the number of households surveyed per upazila
bgd_geo_2014 <- bgd_geo_2014 %>%
  group_by(ADM3_PCODE) %>%
  mutate(num_unique_hhid = n_distinct(hhid)) %>%
  ungroup()

bgd_geo_2014$upazila.code <- substring(bgd_geo_2014$ADM3_PCODE, 3)
View(bgd_geo_2014)

# Merging this new variable with the dataset we are using for the FH model
new_df <- merge(new_df, bgd_geo_2014[, c("upazila.code", "num_unique_hhid")], by = "upazila.code", all.x = TRUE)

cleaned_df <- distinct(new_df, upazila.code, .keep_all = TRUE)

View(cleaned_df)
```

Now we estimate an initial model using the arcsin transformation discussed above whilst still using the geospatial variables which were selected by an initial AIC/BIC variable selection.

```{r}

fh_arcsin <- fh(fixed = binary_wealth ~ min.osm_dst_waterway_100m_2016_BGD + 
    max.bgd_women_2020 + max.bgd_women_of_reproductive_age_15_49_2020 + 
    max.esaccilc_dst_water_100m_2000_2012_BGD + mean.X201501_Global_Travel_Time_to_Cities_BGD + 
    mean.esaccilc_dst_water_100m_2000_2012_BGD + mean.osm_dst_road_100m_2016_BGD + 
    sum.X201501_Global_Travel_Time_to_Cities_BGD + sum.bgd_children_under_five_2020 + 
    sum.bgd_general_2020 + sum.bgd_men_2020 + sum.bgd_women_of_reproductive_age_15_49_2020 + 
    sum.bgd_youth_15_24_2020 + sum.osm_dst_road_100m_2016_BGD + 
    sum.Nightlights_data + stdev.esaccilc_dst130_100m_2014_BGD + 
    count.X201501_Global_Travel_Time_to_Cities_BGD + count.X202001_Global_Motorized_Travel_Time_to_Healthcare_BGD + 
    min.esaccilc_dst150_100m_2014_BGD, vardir = "new_df$se.GVF^2",
          combined_data = cleaned_df, domains = "upazila.code", method = "ml",transformation = "arcsin", backtransformation = "bc", eff_smpsize = "num_unique_hhid",
          MSE = FALSE, mse_type = "boot", B = 50)

summary(fh_arcsin)
```

::: callout-note
We have initially have set MSE = FALSE above to in order to speed the model stepwise selection step up (the next step) but we will be changing MSE back to true in step 6 when we compute the out of sample estimates.
:::

```{r}
lev1 = fh_arcsin$model$real_residuals
lev2 = fh_arcsin$model$random_effects
qqnorm(lev1)
qqnorm(lev2)                   
```

```{r}
summary(fh_arcsin$ind$FH)
```

Now we can see that both Q-Q plots have close to linear 45 degree relationships. Furthermore, when looking at the second diagnostic model, the range of estimates for the variable 'binary_wealth' is between 0 and 1 which is much more appropriate for a proportion. The mean is close to the value of 0.2 which also makes sense since the variable is calculated by the number of households in the bottom quintile of wealth in the country. Note this may not be exactly equal to 0.2 since population could vary between upazilas and this is just the average of all values without any weighting to take account of population. Now we are satisfied that this transformation satisfies the model assumptions we can carry on with the remaining modelling steps.

We now use the step function again, this time using the BIC criterion to see if additional variables can be dropped from the model. This is again following the principle of parsimony to prevent overfitting when the model begins to do out of sample prediction.

::: callout-tip
This step may take a long time to estimate so be patient!
:::

```{r}
step(fh_arcsin, criteria = "BIC") 
```

We then use the variables that are left by this selection process to create our final arcsin model. This is the model we use to produce our out of sample estimates of poverty.

### Step 6: FH model prediction for out of sample estimates

After following the previous step and dropping the geospatial covariates as suggested by the stepwise selection above, we are left with the following model.

```{r}

fh_arcsin_final <- fh(fixed = binary_wealth ~ min.osm_dst_waterway_100m_2016_BGD + 
    max.bgd_women_2020 + max.bgd_women_of_reproductive_age_15_49_2020 + 
    max.esaccilc_dst_water_100m_2000_2012_BGD + mean.X201501_Global_Travel_Time_to_Cities_BGD + 
    mean.esaccilc_dst_water_100m_2000_2012_BGD + mean.osm_dst_road_100m_2016_BGD + 
    sum.X201501_Global_Travel_Time_to_Cities_BGD + sum.bgd_children_under_five_2020 + 
    sum.bgd_general_2020 + sum.bgd_men_2020 + sum.bgd_women_of_reproductive_age_15_49_2020 + 
    sum.bgd_youth_15_24_2020 + sum.osm_dst_road_100m_2016_BGD + 
    sum.Nightlights_data + stdev.esaccilc_dst130_100m_2014_BGD + 
    count.X202001_Global_Motorized_Travel_Time_to_Healthcare_BGD + 
    min.esaccilc_dst150_100m_2014_BGD, vardir = "new_df$se.GVF^2", 
    combined_data = cleaned_df, domains = "upazila.code", method = "ml", 
    transformation = "arcsin", backtransformation = "bc", eff_smpsize = "num_unique_hhid", 
    MSE = TRUE, mse_type = "boot", B = 50)
summary(fh_arcsin_final)
```

::: callout-note
We use 50 bootstraps here but in a real case study you may want to use 200 or even 250. We have also now set the MSE as true since we are going to be using this model for out of sample prediction.
:::

The summary includes a lot of useful information. It also now gives the adjusted R-squared as 0.61. This shows the transformed model is an improvement compared to the raw model.

We now estimate the model-based point estimates again.

```{r}
est = estimators(fh_arcsin_final)
summary(est$ind$FH)
```

Unlike earlier, this estimation now shows that there are no negative predicted estimates of the variable of interest 'binary_wealth'.

We now show graphically the results of this estimation to also graphically show that the model fit is sufficient and the assumptions are satisfied. This function gives 4 output plots to help show that the normality assumption has not been violated. The four plots are as follows:

1.  Normal Q-Q plot of standardized residuals.

2.  Normal Q-Q plot of random effects

3.  Kernel densities of distribution of standardized residuals. We want these next two to be close to the standard normal distribution.

4.  Kernel densities of distribution of random effects.

We want the plots 1 and 2 to be as close to a 45 degree line as possible to show normality of the errors for both fixed and random effects. Plots 3 and 4 will hopefully resemble bell curves. If both plots resemble the normal distribution then it provides further evidence that assumptions of normality for both residuals and the random effects are met.

```{r}
plot(fh_arcsin_final)
```

### Step 7: Comparison between direct estimates and model estimates

Ideally, model-based point estimates and variances should be consistent with the direct estimates from the survey and their respective variances which we calculated in step 2. We should also expect that the use of model-based methods will produce estimates with improved precision compared to direct estimates.

This command gives the first 6 results just so you can get a general impression that there is an improved precision when using FH estimates vs direct estimates but the values are also very similar. We are also able to see that for domain 100447, which was out of sample in the DHS survey (hence the NA), we now have estimates for the variable 'binary_wealth' through using the FH model!

```{r}
head(estimators(fh_arcsin_final,
                MSE = TRUE))
```

We are now also able to graphically compare direct estimates to model based estimates by using the plot command.

The plots below compare the direct estimates against the model-based point estimates. The results show that model-based estimates are correlated well with direct estimates.

```{r}
compare_plot(fh_arcsin_final) 
```

Despite this relationship passing the 'eye-test' we then also carry out a Brown test to test this relationship computationally. This command carries out a Brown test to test the Null hypothesis that EBLUP estimates do not differ significantly from the direct estimates

```{r}
compare(fh_arcsin_final)
```

The output of this command shows that the correlation between the two sets of point estimates is 0.78. In addition, this command performs a statistical test on the differences between the direct and model-based point estimates. The p-value of the test is 1 indicating that the null hypothesis of no difference between the two sets of estimates is not rejected.

**Benchmark the estimates here if required**

This is useful for statistical offices if they want estimates to sum to official figures for reporting.

### Step 8: Visualising the results on administrative level 3 (Upazila) scale:

Congratulations! We have now finished generating the poverty estimates using the FH model and all that is left is to display this information on a beautiful map!

First install and load necessary packages

```{r}
library(ggplot2)
library(maptools)
library(rgeos)
library(ggmap)
library(scales)
library(RColorBrewer)
library(maps)
library(sf)
library(raster)
```

Now we create a new variable named est which includes the both direct and FH estimates from our final model as selected above.

```{r}
est = estimators(fh_arcsin_final,
                 MSE = FALSE)

attach(est)
```

We now set our working directory to where we have downloaded our Bangladesh administrative level 3 shape file and download it to R. This shapefile is the same as what we used in step 3 of section A of the guide as well as section B step 2.

```{r}
setwd("/Users/nicklindsay/Downloads/bgd_adm_bbs_20201113_SHP")

# Reading Bangladesh shapefile at administrative level 3
bgd_admin3 <- st_read("bgd_admbnda_adm3_bbs_20201113.shp")

extent(bgd_admin3)
```

The extent command is a quick check to show the minimum and maximum x and y coordinates that the shapefile contains.

OUR CODE

```{r}
setwd("C:/Users/nickl/iCloudDrive/Desktop/Oxford Policy Management/Bang_Rep/Admin_level3")

# Reading Bangladesh shapefile at administrative level 3
bgd_admin3 <- st_read("bgd_admbnda_adm3_bbs_20201113.shp")

```

We then assign this shape file a name and use the fortify function in order to get it in to a format that can be used for data visualisation. The fortify function converts a shapefile's geometry into a dataframe that includes columns such as "long" or "lat" representing longitude and latitude coordinates of geometries which makes it into a compatible format for plotting with packages in R such as ggplot.

```{r}
setwd("/Users/nicklindsay/Downloads/bgd_adm_bbs_20201113_SHP")
upazila.shp <- readShapeSpatial("bgd_admbnda_adm3_bbs_20201113.shp")
upazila.shp.f <- fortify(upazila.shp)
```

We now need to prepare the data in order to merge the shape file and the estimates for the model so we need to find a common variable or order of the observations.

We therefore create a new variable using the 'est' estimators and ordering the observations so we are able to match it with the arcsin_model version of the Fay-Herriot model we generated above.

```{r}
est.maps <- data.frame(est) %>%
  mutate(ADM3_CODE = Domain)
```

We then also order the list of upazilas by ADM3_CODE

```{r}
est.maps <- est.maps[order(est.maps$ADM3_CODE),]
                    
est.maps$id <- seq(1,
                   nrow(est.maps), 1)                      

```

Note that now there is a column called ADM3_CODE, the observations are arranged in order of this variable. And then they have been assigned an ID number relative to this order

We now just need to merge the shape file with the data frame which includes both the direct and FH estimates

```{r}
est.shp.coef <- merge(upazila.shp.f,
                      est.maps, by = "id",
                      all.x = TRUE)

final.est.plot <- est.shp.coef[order(est.shp.coef$order),]
```

**Map of direct estimates**

The commands for creating the maps of direct estimates vs FH estimates are almost identical. To create the visualisation for the direct estimates you use the argument "fill = Direct", compared to "fill = FH" for the FH estimates. You are also able to adjust the title of this map under the name argument, technically the estimation is showing the probability a household in that upazila is in the bottom quintile of wealth distribution across the country. You are also easily able to adjust the scale of the plot, for example setting pretty_breaks(n=10) if you want the scale at 0.1 (1/10) intervals of probability instead of what it is currently set at 0.2 (1/5).

```{r}
ggplot() + geom_polygon(data = final.est.plot,
                        aes(x = long, y = lat,
                            group = group,
                            fill = Direct),
                        color = "black",
                        linewidth = 0.25) + coord_map() +
  scale_fill_distiller(name = "% Households in Poverty",
                       palette = "Greens",
                       direction = 1,
                       breaks = pretty_breaks(n = 5)) +
  theme_nothing(legend = TRUE)
```

::: callout-note
When you look at this map you can clearly see the 179 out of sample areas represented by the regions in grey.
:::

**Map of FH estimates**

To create the visualisation for the FH estimates you use the argument "fill = FH"

```{r}
ggplot() + geom_polygon(data = final.est.plot,
                        aes(x = long, y = lat,
                            group = group,
                            fill = FH), color = "black",
                        linewidth = 0.25) + coord_map() +
  scale_fill_distiller(name = "% Households in Poverty",
                       palette = "Greens",
                       direction = 1,
                       breaks = pretty_breaks(n = 5)) +
  theme_nothing(legend = TRUE)
```

Here all the previously out of sample areas in grey are now shown with their poverty rates as estimated by the Fay-Herriot model. The FH model has used the most statistically significant geospatial covariates to fit a model using data from the DHS household survey as a ground-truth measure of poverty to then predict poverty for out-of-sample areas.

### Exporting the results to your medium of choice

If, in addition to the visualisations, you want the raw data for further analysis then it is very easy to export the data to an Excel file. You can then carry out further data-processing on your medium of choice. This command will export the model summary as well as direct and model-based point and uncertainty estimates. The file will be saved in your working directory.

```{r}
# Exporting estimates to an excel file 
write.excel(fh_arcsin_final,
            file = "fh_arcsin.xlsx",
            MSE = TRUE)
```

# Conclusion

This guide has run through how to produce poverty estimates for out of sample areas using the Fay-Herriot model in R. Section A of the guide details how to collect the geospatial data from various online sources and harmonise the information into one single dataset. Section B explains the estimation procedure and the steps that must be taken to ensure precise, robust estimates. The combination of household data and geospatial data rather than census data (which is the method that has traditionally been used) enables the user to produce poverty estimates with more frequency and at a lower cost. Furthermore the public availability of geospatial data ensures that this method is available to a wider audience of agencies which should add valuable momentum to the poverty reduction conversation.

**The motivation for this guide is therefore twofold:**

1.  From an academic standpoint:

    -   Closing data gaps by providing estimates for previously hard to reach out of sample areas.

    -   Integrating data by showing how to combine geospatial and household data.

    -   Improving precision of estimates by providing methodology to produce a robust model that compares favourably to other previous estimation techniques

2.  From a policy standpoint:

    -   Lowering costs for government agencies, NGOs and thinktanks.

    -   Influencing and improving policy decisions and social protection programs in order to drive poverty reduction. This is done by providing more frequent statistics at higher precision than previously available which allows policy makers to make more informed decisions.

**Caveats**

Despite these clear benefits, small area estimation is not a panacea. The out of sample estimates are only as good as your model, the assumptions the model makes and the underlying data that goes into the model. As a result these are some of the key considerations you need to take into account before undertaking small area estimation:

**Sample size:** The Fay-Herriot model assumes a large sample size for both the target and auxiliary variables. If the sample size in the small area is small, the model's assumptions may be violated, leading to unreliable estimates.

**Model specification:** Choosing an appropriate model specification is crucial. The Fay-Herriot model assumes a linear relationship between the small area and auxiliary variables. If this assumption is not met, the model may provide biased estimates. It's important to assess the linearity assumption and consider alternative model specifications if needed.

**Auxiliary variable availability:** The Fay-Herriot model relies on accurate and relevant auxiliary variables that are correlated with the small area variable of interest. If suitable auxiliary variables are not available or are not strongly correlated with the target variable, the model's effectiveness may be limited.

**Assumption of homoscedasticity:** The Fay-Herriot model assumes that the variance of the small area random effects is constant across all areas. If there are substantial differences in the variance between areas, the model may not capture the heterogeneity adequately, leading to biased estimates.

**Model extrapolation:** The Fay-Herriot model relies on borrowing strength from neighboring areas to estimate small area parameters. However, if the small area is significantly different from the neighboring areas, the model's estimates may not accurately reflect the true values.

It is therefore vitally important to perform model diagnostics throughout the process such as assessing the goodness-of-fit and assessing the predictive power of the model in order to evaluate the appropriateness of the Fay-Herriot model. Failure to perform proper diagnostics may lead to incorrect inferences.

It's important to carefully consider these caveats and assess the suitability of the Fay-Herriot model in the specific context of your small area estimation problem. Alternative methods should be explored if the assumptions of the Fay-Herriot model are not met or if there are specific considerations unique to your data.

# Glossary

**BIC selection criterion:** BIC stands for Bayesian Information Criterion, which is a statistical measure used for model selection. It balances the goodness of fit of a model with its complexity to find the most appropriate model. The BIC selection criterion penalizes models with more parameters, helping to avoid overfitting and select simpler models.

**Homoskedasticity:** Homoskedasticity refers to a statistical assumption that the variance of the error term in a regression model is constant across all levels of the predictor variables. In the context of small area estimation using the Fay-Herriot model, homoskedasticity assumes that the variance of the random effects (unobserved area-specific effects) is constant across different small areas.

**Generalised Variance Functions (GVF):** Generalized Variance Functions are statistical functions used to estimate the variance of a variable, if it is not available in the survey.

**Linearity assumptions:** Linearity assumptions refer to the assumption that the relationship between the predictor variables and the response variable is linear. In the context of the Fay-Herriot model, linearity assumptions imply that the small area estimation model assumes a linear relationship between the auxiliary variables (used for prediction) and the variable being estimated in each small area.

**Normality assumptions:** Normality assumptions refer to the assumption that the error term in a statistical model follows a normal distribution. In small area estimation using the Fay-Herriot model, normality assumptions imply that the random effects and the residuals in the model are normally distributed. This assumption allows for valid statistical inference and accurate estimation of confidence intervals for small area estimates.

**Principal Components Analysis (PCA):** In the context of creating a wealth index using survey questions, Principal Components Analysis (PCA) is a statistical technique used to derive a composite score that represents the underlying dimension of wealth or socioeconomic status.

**Overfitting:** Overfitting refers to a situation in statistical modeling where a model fits the training data too closely, capturing random noise and idiosyncrasies of the data rather than the underlying true relationship. Overfitting can lead to poor generalization and inaccurate predictions when the model is applied to out of sample data.

**Bootstrap:** Bootstrapping is a resampling technique used for estimating the sampling distribution of a statistic by repeatedly sampling with replacement from the original dataset. In small area estimation, the bootstrap is used to estimate the variance of small area estimates and construct confidence intervals.

**Brown Test:** The Brown Test is a statistical test used to assess the adequacy of a model in small area estimation. It examines whether the model assumptions, such as linearity and homoscedasticity, hold reasonably well. The Brown Test helps ensure the validity of the model and the reliability of the small area estimates.

**Upazila:** Upazila is a geographical administrative unit used in Bangladesh. It is a sub-district-level division.

**Design Effect:** Design Effect is a measure used to quantify the effect of clustering or stratification in sample surveys. It accounts for the correlation among observations within the same cluster or stratum and affects the precision of the estimates. A higher design effect indicates a less efficient sample design.

**Intra-Cluster Correlation Coefficient (ICC):** The Intra-Cluster Correlation Coefficient measures the degree of similarity or correlation among observations within the same cluster. In small area estimation, ICC helps capture the variation within clusters and is crucial for estimating the sampling variance and designing efficient surveys.

**Principle of Parsimony:** The Principle of Parsimony, also known as Occam's Razor, suggests that when multiple models can explain the data equally well, the simplest model should be preferred. In small area estimation, the principle of parsimony encourages selecting models with fewer parameters to avoid overfitting.

**Q-Q Plot:** Q-Q Plot, short for Quantile-Quantile Plot, is a graphical tool used to assess the distributional similarity between two datasets. In small area estimation, a Q-Q plot can be used to compare the observed quantiles of the small area estimates with the quantiles expected under a theoretical distribution, such as the normal distribution.

**MSE (Mean Squared Error):** Mean Squared Error is a measure of the average squared difference between the estimated values and the true values. In small area estimation, MSE is used to assess the accuracy and precision of the small area estimates, with lower MSE indicating better performance.

**R-squared:** R-squared, also known as the coefficient of determination, measures the proportion of the variance in the response variable that is explained by the predictors in a regression model.

**EBLUP Estimates:** Empirical Best Linear Unbiased Prediction estimates. EBLUP estimates strike a balance between the sample data and the auxiliary information to generate reliable estimates at the small area level.

**Stratification:** Stratification refers to the process of dividing the target population into distinct sub-groups or 'strata' based on certain characteristics or attributes relevant to the study. The method ensures you capture the diversity within the population in your sample while ensuring more accurate estimates within each subgroup

**Noise:** Noise refers to the random or unstructured variation in a variable's values that is not related to the underlying phenomenon being measured. Noise can arise from various sources such as measurement errors, environmental factors, human error or other random influences.
