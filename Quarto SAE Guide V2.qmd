---
title: "Small Area Estimation Guide"
format: html
editor: visual
---

# **Guide to Carry Out Small Area Estimation in R**

Oxford Policy Management 2023

# Contents Page

### Introduction:

Page number:

### Estimation:

Page number:

### Conclusion / Caveats:

Page number:

### Glossary

Page number:

# Introduction

**Why is it so important to learn more about the spatial distribution of poverty?**

-   More than 700 million people or 9.3% of the population live on less than \$2.15 a day.
-   Approximately half the world's population live on less than \$6.85 a day.
-   Despite global inequality declining, inequality within countries is rising so it is increasingly important for policymakers to appreciate the spatial distribution of wealth across a whole country.

Social protection programs and appropriate policy decisions are important drivers of poverty reduction but they need suitable data in order to effectively target those who need help the most. Policy makers therefore require the most current statistics as well as figures that give an idea of poverty distribution across the whole country. This method of small area estimation can help solve both of these issues.

Poverty estimates are a crucial measure for policymakers and development practitioners to understand the extent of poverty in a country and design effective poverty alleviation programs. This guide will tell you how to produce these estimates.

**The Fay-Herriot (FH) Model**

The FH model in R can be used to produce out-of-sample poverty estimates.

The FH model uses a combination of household data and geospatial data in order to generate estimates of poverty rates.

It does this by fitting a model on the in-sample observations by using the household survey responses regarding poverty and learning which geospatial covariates have the most predictive power on the asset index. The model can then estimate the poverty rates for out of sample households using just the geospatial data.

This technique is demonstrated by [Blumenstock et al., (2022)](https://pubmed.ncbi.nlm.nih.gov/35017299/), who develop relative wealth and poverty microestimates for all 135 low and middle income countries (LMICs). Furthermore,

**Why do we use geospatial data rather than census data?**

-   Previously, small area estimation poverty estimates were generated using a combination of household survey and census data, which is only collected every 10 years. However, a combination of household survey data and geospatial data allows you to generate poverty estimates much more frequently than was possible previously.
-   Geospatial data is open source and can provide more precise and up-to-date information about covariates than census data. This also makes the estimation process cheaper if you use geospatial data.
-   Geospatial data does not have a problem of finding statistics in rural areas which are often hard to reach in traditional surveys.

**Geospatial has all these advantages whilst still offering sufficient predictive power**

-   Geospatial data is available at a high spatial resolution, which can be matched with the geo-location of households in order to allow prediction of household welfare from the co-ordinates.
-   This information can then be used to estimate poverty rates for that household by using geospatial information such as land use, population demographics and climatic variables.
-   The FH model using geospatial data, when compared to ground-truth data in Togo using a nationally representative sample of 6,172 households found that the model explained 84% of the variation in wealth. This compares favourably to other state-of-the-art models.

**The guide**

In this guide, we will take you through the steps of using the Fay-Herriot model in R to carry out small area estimation to predict poverty rates. This will enable you to replicate this prediction in other developing countries using just geospatial data and a household survey that includes a wealth index. We use the DHS household survey that includes data on over 90 different countries for ease of replication in other countries for the user. We will explain how the model works, the data we use, and why we might choose to use this method over traditional approaches. By the end of this guide, you will be able to produce accurate and up-to-date poverty estimates for small areas in developing countries, which can inform policy decisions and improve poverty alleviation efforts.

In this guide we use the Fay-Herriot model vs other predictive models since the FH model estimates poverty rates using the agglomerated average wealth index of each area (and is therefore known as an area-level model) rather than other models such as the EBP (empirical best predictor) which estimates the poverty rates by generating an estimate for every single household (known as unit-level). This makes the results easier to replicate in a large variety of countries since there is often a problem in obtaining unit-level data due to data protection laws.

**Why we use the relative wealth index as our measure of poverty**

We use the DHS "relative wealth index" as our ground-truth measure of wealth and poverty. This means that our machine-learning algorithms are being trained to reconstruct a specific, asset-based relative wealth index - albeit at a much finer spatial resolution and in areas where DHS surveys did not occur. This is because we believe the DHS version of a relative wealth index is the best publicly available instrument for consistently measuring wealth across a large number of developing countries.

The wealth index is a composite measure of a household's cumulative living standard. The wealth index is calculated using easy-to-collect data on a household's ownership of selected assets, such as televisions and bicycles; materials used for housing construction; and types of water access and sanitation facilities. Generated with a statistical procedure known as principal components analysis, the wealth index places individual households on a continuous scale of relative wealth. However, it posits a specific, asset-based definition of wealth that does not necessarily capture a broader notion of human development.

Our decision to focus on estimating asset-based wealth, rather than a different measure of socioeconomic status, was motivated by several considerations:

-   In developing economies, where large portions of the population do not earn formal wages, measures of income are notoriously unreliable. Instead, researchers and policymakers rely on asset-based wealth indexes or measures of consumption expenditures.
-   Between these two, wealth is much less time consuming to record in a survey. As a result, wealth data is more commonly collected in a standardized format for a large number of countries.
-   This therefore lends itself to the FH method being easily replicated and reproduced in a wide variety of case studies in less developed economies around the world.

### Steps of the guide

Using the Fay-Herriot model as a method to produce out of sample estimates of poverty initially seems like quite an involved process. However, the main steps are quite simple. You first gather your geospatial data and download the DHS household survey responses from online. You then calculate the direct estimates of poverty in the country using the DHS household survey. The reason you cannot simply use the numbers in the survey is because DHS employs a sampling strategy to gather their data and this needs to be accounted for and the numbers need to be adjusted in order to make the estimates as precise as possible. You then select the most significant geospatial covariates at predicting poverty and use them in a Fay-Herriot model to generate your out of sample poverty estimates. These estimates are then used to visualise the spatial distribution of poverty in the country.

The remainder of the steps in the process are used to ensure the robustness and precision of the estimates. The FH model has a few assumptions that you need to check are not violated, especially if you are using the model to predict out of sample estimates. It is therefore key that these steps are followed if the statistics you produce are intended to be used to guide policy decisions in the future.

Step 1: Gathering the geospatial data and compiling into a harmonised dataset.

You can either start with the geospatial co-ordinates you generated in section A or use our pre-processed dataframe which is available [here.]{.underline} Part 1 of our guide explains how to gather the geospatial data in detail.

Step 2: Generating the direct estimates of the wealth index and their sampling variance for the in-sample estimates.

Surveys often employ complex sampling designs, with elements of stratification and multistage sampling with unequal probabilities. In order to account for this, we need to calculate direct estimates and estimates of their variances for the wealth index in the DHS household dataset. This is in order to prevent bias being introduced into the estimation.

For this we use the survey() package in R.

Step 3: Merging the geospatial data, the household data and your direct estimates.

Now we need to combine the geospatial data from part 1 of the guide, the DHS household survey and the direct estimates and their variances that you generated in step 2 of the process. In order to combine this information into one big dataset we need a unique common identifier and for the we use the unique domain identifier and merge the observations using this.

Step 4: Selecting the geospatial covariates with the most predictive power

The initial large dataframe with over 200 geospatial variables is unwieldy and following the 'principle of parsimony' we want fewer variables. We therefore initially trim the model to include fewer covariates. This helps to prevent issues such as overfitting. For this we use a selection criterion (AIC/BIC/KIC) to find only the geospatial covariates with the highest predictive power on predicting the wealth estimates.

We perform a BIC stepwise selection criterion using all of the available geospatial covariates to see which 10-15 variables perform the best in predicting the level of wealth in an area.

Step 5: Generating the model and checking model assumptions.

We now use the geospatial covariates selected in step 4 by the selection criterion in a cursory model in order to test the key assumptions of the model and assess the predictive power of the model. For example, the model assumes normality of the error terms which, if violated, introduces bias into the estimation so it is necessary to show that all the necessary assumptions hold.

If the assumptions do not hold with the model in its current format then it is important to carry out a transformation on the target variable. Log transformations are traditionally popular with income data in order to mitigate issues such as skewed data. This transformation stabilises the variance and makes the relationship with other variables more linear. We use a variety of visual aids such as plots as well as regression tests to ensure that we are satisfied that these assumptions now hold.

Step 6: Carry out the FH model prediciton for our out of sample estimates.

In this step we carry out the FH modelling procedure in order to generate our out of sample wealth estimates. The FH model adopts a bootstrap approach in order to predict out of sample estimates. This is an iterative resampling method which provides estimates of variability and uncertainty without requiring strong assumptions about the data distribution.

Step 7: Comparison of FH model estimates with the ground-truth direct estimates.

In this step we compare the direct point poverty estimates we generated in step 2 using the ground-truth data with the FH model estimates that we generated in step 6. This is done both graphically and with a Brown test. This gives some indication of the precision of the model. If we are satisfied with the similarity of the estimates, we can move on to the final stage.

Step 8: Visualisation of the results.

We then visualise the results. To do this we have to download a Bangladesh shapefile from online which has the country co-ordinates and includes the polygon shape for each upazila. We merge the dataset using the unique upazila district code and then visualise the relative wealth in each domain by using a gradient colour scheme.

# Estimation

### Using the Fay Herriot model to predict poverty in Bangladesh

First you need to install and load necessary packages for the estimation:

```{r}
library(survey)
library(nlme)
library(emdi)
library(dplyr)
library(sae)
```

### Step 1: Gathering your geospatial data\]

This step is covered extensively in part 1 of the guide. Ensure to save this dataset to your working directory since it is used throughout the process.

Alternatively, for practice, you can use our pre-processed estimates downloadable *here*.

### Step 2: Generating direct estimates and their sampling estimates

This step is necessary in order to generate our poverty estimates for the in sample areas. Since the DHS survey employs a sampling strategy we cannot take the values they produce at face-value and first have to adjust them to make them more representative of the whole sample.

First of all you have to generate the unique domain identifier for each observation so that you are able to identify each observation by both its upazila as well as its cluster.

This new dataframe is represented by Ida's HH + Geospatial data.

It is then merged with the sampling weights from the household data.

```{r}

setwd("C:/Users/nickl/iCloudDrive/Desktop/Oxford Policy Management/Bang_Rep")

library(readr)
Ida_s_HH_Geospatial_Data <- read_csv("Ida's HH + Geospatial Data.csv")
View(Ida_s_HH_Geospatial_Data)

library(haven)
BDHR7RFL <- read_dta("BDHR7RFL.DTA")
View(BDHR7RFL)

Ban_data <- cbind(Ida_s_HH_Geospatial_Data, hv022 = BDHR7RFL$hv022)

View(Ban_data)

```

Next we want to generate our variable of interest. In our case we use the variable 'prop_wealth_1'. This is the probability that a household will be in the lowest quintile of wealth across the whole survey. By definition 20% of households across the country will be in this bottom quantile but you will be able to see which areas are poorer by seeing which areas have a higher probability of a household being in this bottom quintile of wealth.

```{r}

## Making the poorest variable 

install.packages("dplyr")
library(dplyr)

## Calculating our version of the poorest variable "prop_wealth_1"

proportion <- Ban_data %>%
  group_by(ADM3_PCODE) %>%
  summarize(prop_wealth_1 = sum(Wealth_quint_rank == 1) / n())

summary(proportion)
Ban_data <- left_join(Ban_data, proportion,by = "ADM3_PCODE")

View(Ban_data)
```

We now look at creating variables to determine how many clusters are in each upazila because depending on this, the estimates will be more or less precise. The number of clusters in each upazila is necessary so that we know which method we will use to calculate our direct estimates in the next step.

```{r}

## How many clusters are in each upazila 

Ban_data <- Ban_data %>%
  group_by(ADM3_PCODE) %>%
  mutate(num_clusters = n_distinct(DHSCLUST))


frequency_table <- table(Ban_data$num_clusters)

print(frequency_table)

# Calculate the count of clusters per Upazila
upazila_cluster_count <- Ban_data %>%
  group_by(ADM3_PCODE) %>%
  summarize(num_clusters = n_distinct(DHSCLUST))

# View the summary table
print(upazila_cluster_count)

# Summarize the count of clusters per Upazila
cluster_summary <- table(upazila_cluster_count$num_clusters)

# View the summary table
print(cluster_summary)
```

Finally we can generate the direct estimates and the sampling variances. The method is different if there is 1 cluster per upazila since by definition there will be the same variation in the cluster as in the upazila as a whole. Therefore we use a design effect if there is only one upazila in the cluster, if there is more than 1 cluster in the upazila we are able to calculate using the survey() package.

```{r}
library(survey)
DHS.design <- svydesign(ids = ~ DHSCLUST,
                        weights = ~ Sample_Weight,
                        strata = ~ hv022,
                        data = Ban_data)


de <- svyby(~ prop_wealth_1,
            by = ~ADM3_PCODE,
            design = DHS.design,
            svymean)

summary(de)

### Estimation of the
### design effect for
### upazilas with only one
### cluster

library(nlme)

fit.rs <- lme(prop_wealth_1 ~
                1, random = ~1 |
                DHSCLUST, method = "ML",
              data = Ban_data[Ban_data$num_clusters ==
                                1, ])

plot(fit.rs)
vcomp <- as.numeric(VarCorr(fit.rs))

print(VarCorr(fit.rs))


deff.1clus <- 1 + (4 -
                     1) * vcomp[1]/(vcomp[1] +
                                      vcomp[2])
print(deff.1clus)


### Estimation of the
### standard error for
### upazilas with one
### cluster


design.STSRS <- svydesign(id = ~1,
                          weights = ~weight,
                          strata = ~strat.se,
                          data = dhs.data)
de.STSRS <- svyby(~poorest,
                  by = ~upazila.code,
                  design = design.STSRS,
                  svymean)
de.STSRS <- dplyr::select(de.STSRS,
                          upazila.code, se.STSRS = se)
de <- merge(de, sizes,
            by = "upazila.code") %>%
  merge(de.STSRS, by = "upazila.code") %>%
  mutate(se.imp = ifelse(nclus ==
                           1, se.STSRS *
                           sqrt(deff.1clus),
                         se))
```

### Step 3: How to combine your datasets. Getting your geospatial data, direct estimates and survey data in the same dataframe.

First you load the necessary dataset from where you have it saved.

```{r}
setwd("/Users/nicklindsay/Downloads")

data_area_level <- get(load("final.cov.est.new-1.Rdata"))

data_area_level = cbind(data_area_level, data_area_level$se.GVF^2)

attach(data_area_level)

```

This also calculates the variance of the GVF (Generalised Variance Function) variable and attaches it to the dataset since it is an important element in the estimation.

We use the column bind command to merge two dataframes together. This new dataframe now includes the variable for "vardir" which is standard errors squared.

### Step 4: Covariate selection for initial model.

This step follows the principle of parsimony in order to slim down the model to find only the geospatial covariates that have the highest predictive power. To do this we first create a new dataframe without the variables that were used to create the wealth index. Since these will clearly be highly correlated with our 'poorest' indicator of interest and we do not want these to be in the model. Also they are obviously not available for the out of sample data points.

```{r}
new_df2 <- data_area_level[, c(2:210, 223)]

```

We then fit a full linear regression model using the new dataframe

```{r}
full_model2 <- lm(poorest ~ ., data = new_df2)
```

We then perform stepwise selection based on BIC criterion to narrow down the number of variables from 200+ to around 15

```{r}
stepwise_model2 <- stepAIC(full_model2, direction = "both", trace = FALSE, k = log(nrow(new_df2)))
```

We can then view the selected variables which we will use in our cursory model.

```{r}
summary(stepwise_model2)
```

### Step 5: Fitting the model

Now we estimate an initial cursory model using the variables selected by an initial AIC/BIC variable selection.

```{r}
new_fh_final <- fh(fixed = poorest ~ CHILU5.SUM + MEN.MEAN + WOMEN.MIN + WOMEN.SUM +
          WOMENREP.MIN + WOMENREP.SUM + YOUTH1524.MIN + YOUTH1524.MEAN +
          VIIRS.SUM + DSTWATWAY.MAX + DSTCOAST.MEAN, vardir = "data_area_level$se.GVF^2",
          combined_data = data_area_level, domains = "upazila.code", method = "ml",
          transformation = "arcsin", backtransformation = "bc", eff_smpsize = "nhog",
          MSE = TRUE, mse_type = "boot", B = 50)

summary(new_fh_final)
```

First of all we carry out the estimation with no transformation on a raw scale to see if the model offers reasonable predictive power.

This is just an illustrative example using only the variables that are later selected using the AIC/ BIC.

You can then run a couple of model diagnostic regressions to see how the raw scale fits the data.

```{r}
fh_raw_scale <- fh(fixed = poorest ~
                     CHILU5.SUM + MEN.MEAN +
                     WOMEN.MIN + WOMEN.SUM +
                     WOMENREP.MIN +
                     WOMENREP.SUM +
                     YOUTH1524.MIN +
                     YOUTH1524.MEAN +
                     VIIRS.SUM + DSTWATWAY.MAX +
                     DSTCOAST.MEAN,
                   vardir = "data_area_level$se.GVF^2",
                   combined_data = data_area_level,
                   domains = "upazila.code",
                   method = "ml", MSE = TRUE,
                   mse_type = "analytical")
                   
summary(fh_raw_scale)                   
```

When looking at the model summary the adjusted R-squared is 0.52. This might seem like reasonable predictive power.

However, we also need to look at normal probability plots of residuals and random errors to see if there is a clear departure from normality which is carried out below.

These commands give 2 output plots showing: 1. Normal Q-Q plot showing level 1 residuals 2. Normal Q-Q plot showing the random effects.

```{r}
lev1 = fh_raw_scale$model$real_residuals
lev2 = fh_raw_scale$model$random_effects
qqnorm(lev1)
qqnorm(lev2)                   
```

Especially in output plot 1 this graph resembles more of an exponential growth function rather than a 45 degree line which we would hope for.

The second diagnostic model is important if we are going to use the model for predictive purposes.

This next function will output direct and model-based point estimates and corresponding variances and MSEs provided that the MSE argument was set equal to TRUE when fitting the area- level model.

```{r}
est_raw = estimators(fh_raw_scale,
                     MSE = TRUE)             
summary(fh_raw_scale$ind$FH)
```

We notice that the model-based point estimates include negative predicted values despite the fact that we are modelling a proportion. This is a consequence of using a model that does not fit well and illustrates the importance of using model diagnostics.

As a result in order to better fit the data we use the arcsin transformation. This is common when using income data. Using this transformation requires additional arguments, namely the effective sample size defined by the sample size divided by the design effect and the type of backtransformation.

```{r}
fh_arcsin <- fh(fixed = poorest ~
                  POP.MAX + CHILU5.STD +
                  CHILU5.SUM +
                  ELD60P.STD +
                  MEN.MEAN + MEN.STD +
                  WOMEN.MIN + WOMEN.MEAN +
                  WOMEN.STD + WOMEN.SUM +
                  WOMENREP.MIN +
                  WOMENREP.STD +
                  WOMENREP.SUM +
                  YOUTH1524.MIN +
                  YOUTH1524.MEAN +
                  SLOPE.MAX + SLOPE.MEAN +
                  SLOPE.SUM + SLOPE.MINORITY +
                  VIIRS.MIN + VIIRS.MAX +
                  VIIRS.MEAN +
                  VIIRS.STD + VIIRS.SUM +
                  TOPO.MEAN + DSTWATWAY.MAX +
                  DSTCOAST.MEAN,
                vardir = "data_area_level$se.GVF^2",
                combined_data = data_area_level,
                domains = "upazila.code",
                method = "ml", transformation = "arcsin",
                backtransformation = "bc",
                eff_smpsize = "nhog",
                MSE = FALSE, mse_type = "boot",
                B = c(0,50)) 
```

NB We have initially have set MIC = FALSE above to in order to speed the model selection step up but we will be changing this later.

We now use the step function using the BIC criterion to see if additional variables can be dropped from the mode. Note that this takes a long time to estimate

```{r}
step(fh_arcsin, criteria = "BIC") 
```

We then use the variables that are left by this selection process to create our final arcsin model. This is the model we use to produce our out of sample estimates of poverty.

### Step 6: FH model prediction for out of sample estimates

```{r}
fh_arcsin_final <- fh(fixed = poorest ~ CHILU5.SUM + MEN.MEAN + WOMEN.MIN + WOMEN.SUM +
          WOMENREP.MIN + WOMENREP.SUM + YOUTH1524.MIN + YOUTH1524.MEAN +
          VIIRS.SUM + DSTWATWAY.MAX + DSTCOAST.MEAN, vardir = "data_area_level$se.GVF^2",
          combined_data = data_area_level, domains = "upazila.code", method = "ml",
          transformation = "arcsin", backtransformation = "bc", eff_smpsize = "nhog",
          MSE = TRUE, mse_type = "boot", B = 50)
summary(fh_arcsin_final)
```

Note we use 50 bootstraps here but in a real case study you may want to use 200 or even 250.

The summary includes a lot of useful information. It also now gives the adjusted R-squared as 0.56. This shows the transformed model is an improvement compared to the raw model.

We now estimate the model-based point estimates again.

```{r}
est = estimators(fh_arcsin_final)
summary(est$ind$FH)
```

Unlike earlier, this estimation now shows that there are no negative predicted estimates of the proportion of the poorest households

We now show graphically the results of this estimation. This function gives 4 output plots to help show that the normality assumption has not been violated.

1\. Normal Q-Q plot of standardized residuals. We want these first two to be as close to a 45 degree line as possible.

2\. Normal Q-Q plot of random effects

3\. Kernel densities of distribution of standardized residuals. We want these next two to be close to the standard normal distribution.

4\. Kernel densities of distribution of random effects.

```{r}
plot(fh_arcsin_final)
```

### Step 7: Comparison between direct estimates and model estimates

Ideally, model-based point estimates should be consistent with the direct estimates we calculated in step 2. We should also expect that the use of model-based methods will produce estimates with improved precision compared to direct estimates.

This command gives the first 6 results just so you can get a general impression that there is an improved precision when using FH estimates vs direct estimates

```{r}
head(estimators(fh_arcsin_final,
                MSE = TRUE))
```

We can now graphically compare direct estimates to model based estimates by using the plot command.

The output below plots the direct against model-based point estimates. The results show that model-based estimates are correlated well with direct estimates.

```{r}
compare_plot(fh_arcsin_final) 
```

We can then carry out a Brown test to test this relationship computationally. This carries out a Brown test to test the Null hypothesis that EBLUP estimates do not differ significantly from the direct estimates

```{r}
compare(fh_arcsin_final)
```

The output of this command shows that the correlation between the two sets of point estimates is 0.73. In addition, this command performs a statistical test on the differences between the direct and model-based point estimates. The p-value of the test is 1 indicating that the null hypothesis of no difference between the two sets of estimates is not rejected.

**Benchmark the estimates if required**

Good for statistical offices if they want estimates to sum to official figures for reporting

### Step 8: Visualising the results on administrative level 3 (Upazila) scale:

First install and load necessary packages

```{r}
library(ggplot2)
library(maptools)
library(rgeos)
library(ggmap)
library(scales)
library(RColorBrewer)
library(maps)
library(sf)
library(raster)
```

First of all we create a new variable named est which includes the both direct and FH estimates from our final model as selected above.

```{r}
est = estimators(fh_arcsin_final,
                 MSE = FALSE)

attach(est)
```

We know set our working directory to where we have downloaded our Bangladesh administrative level 3 shape file and download it to R. We also perform a diagnostic review to see a few key features of the shape file.

```{r}
setwd("/Users/nicklindsay/Downloads/bgd_adm_bbs_20201113_SHP")

bgd_admin3 <- st_read("bgd_admbnda_adm3_bbs_20201113.shp")

extent(bgd_admin3)
```

We then assign this shape file a name and use the fortify function in order to get it in to a format that can be used for data visualisation.

```{r}
setwd("/Users/nicklindsay/Downloads/bgd_adm_bbs_20201113_SHP")
upazila.shp <- readShapeSpatial("bgd_admbnda_adm3_bbs_20201113.shp")
upazila.shp.f <- fortify(upazila.shp)
```

We now need to prepare the data in order to merge the shape file and the estimates for the model so we need to find a common variable or order of the observations.

We therefore create a new variable using the 'est' estimators and ordering it in order to match it with the arcsin_model

```{r}
est.maps <- data.frame(est) %>%
  mutate(ADM3_CODE = Domain)
```

We then also order the list of upazilas by ADM3_CODE

```{r}
est.maps <- est.maps[order(est.maps$ADM3_CODE),]
                    
est.maps$id <- seq(1,
                   nrow(est.maps), 1)                      

```

Note that now there is a column called ADM3_CODE, the observations are arranged in order of this variable. And then they have been assigned an ID number relative to this order

We now just need to merge the shape file with the data frame which indludes both the direct and FH estimates

```{r}
est.shp.coef <- merge(upazila.shp.f,
                      est.maps, by = "id",
                      all.x = TRUE)

final.est.plot <- est.shp.coef[order(est.shp.coef$order),]
```

**Map of direct estimates**

To create the visualisation for the direct estimates you use the command fill = Direct

```{r}
ggplot() + geom_polygon(data = final.est.plot,
                        aes(x = long, y = lat,
                            group = group,
                            fill = Direct),
                        color = "black",
                        linewidth = 0.25) + coord_map() +
  scale_fill_distiller(name = "Poorest",
                       palette = "Greens",
                       direction = 1,
                       breaks = pretty_breaks(n = 5)) +
  theme_nothing(legend = TRUE)
```

**Map of FH estimates**

To create the visualisation for the FH estimates you use the command fill = FH

```{r}
ggplot() + geom_polygon(data = final.est.plot,
                        aes(x = long, y = lat,
                            group = group,
                            fill = FH), color = "black",
                        linewidth = 0.25) + coord_map() +
  scale_fill_distiller(name = "Poorest",
                       palette = "Greens",
                       direction = 1,
                       breaks = pretty_breaks(n = 5)) +
  theme_nothing(legend = TRUE)
```

### Exporting the results to your medium of choice

# Conclusion / Caveats

**Sample size:** The Fay-Herriot model assumes a large sample size for both the target and auxiliary variables. If the sample size in the small area is small, the model's assumptions may be violated, leading to unreliable estimates.

**Model specification:** Choosing an appropriate model specification is crucial. The Fay-Herriot model assumes a linear relationship between the small area and auxiliary variables. If this assumption is not met, the model may provide biased estimates. It's important to assess the linearity assumption and consider alternative model specifications if needed.

**Auxiliary variable availability:** The Fay-Herriot model relies on accurate and relevant auxiliary variables that are correlated with the small area variable of interest. If suitable auxiliary variables are not available or are not strongly correlated with the target variable, the model's effectiveness may be limited.

**Assumption of homoscedasticity:** The Fay-Herriot model assumes that the variance of the small area random effects is constant across all areas. If there are substantial differences in the variance between areas, the model may not capture the heterogeneity adequately, leading to biased estimates.

**Spatial correlation:** The model does not explicitly account for spatial correlation among the small areas. If there is spatial dependence present in the data, the model's estimates may be inefficient or biased. Including spatial random effects or using spatial small area estimation methods may be more appropriate in such cases.

**Model extrapolation:** The Fay-Herriot model relies on borrowing strength from neighboring areas to estimate small area parameters. However, if the small area is significantly different from the neighboring areas, the model's estimates may not accurately reflect the true values.

**Model diagnostics:** It is essential to assess the goodness-of-fit and model diagnostics to evaluate the appropriateness of the Fay-Herriot model. Failure to perform proper diagnostics may lead to incorrect inferences.

It's important to carefully consider these caveats and assess the suitability of the Fay-Herriot model in the specific context of your small area estimation problem. Alternative methods should be explored if the assumptions of the Fay-Herriot model are not met or if there are specific considerations unique to your data.

# Glossary

BIC selection criterion Homoskedasticity GVF Linearity assumptions Normality assumptions
